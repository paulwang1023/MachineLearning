{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Download dataset\n!wget \"https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.0/real_or_drawing.zip\" -O real_or_drawing.zip\n\n# Download from mirrored dataset link\n# !wget \"https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.1/real_or_drawing.zip\" -O real_or_drawing.zip\n# !wget \"https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.2/real_or_drawing.zip\" -O real_or_drawing.zip\n\n# Unzip the files\n!unzip -q real_or_drawing.zip","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef no_axis_show(img, title='', cmap=None):\n  # imshow, and set the interpolation mode to be \"nearest\"。\n    fig = plt.imshow(img, interpolation='nearest', cmap=cmap)\n  # do not show the axes in the images.\n    fig.axes.get_xaxis().set_visible(False)\n    fig.axes.get_yaxis().set_visible(False)\n    plt.title(title)\n\ntitles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\nplt.figure(figsize=(18, 18))\nfor i in range(10):\n    plt.subplot(1, 10, i+1)\n    fig = no_axis_show(plt.imread(f'real_or_drawing/train_data/{i}/{500*i}.bmp'), title=titles[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18, 18))\nfor i in range(10):\n    plt.subplot(1, 10, i+1)\n    fig = no_axis_show(plt.imread(f'real_or_drawing/test_data/0/' + str(i).rjust(5, '0') + '.bmp'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\ntitles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\nplt.figure(figsize=(18, 18))\n\noriginal_img = plt.imread(f'real_or_drawing/train_data/0/0.bmp')\nplt.subplot(1, 5, 1)\nno_axis_show(original_img, title='original')\n\ngray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\nplt.subplot(1, 5, 2)\nno_axis_show(gray_img, title='gray scale', cmap='gray')\n\ngray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\nplt.subplot(1, 5, 2)\nno_axis_show(gray_img, title='gray scale', cmap='gray')\n\ncanny_50100 = cv2.Canny(gray_img, 50, 100)\nplt.subplot(1, 5, 3)\nno_axis_show(canny_50100, title='Canny(50, 100)', cmap='gray')\n\ncanny_150200 = cv2.Canny(gray_img, 150, 200)\nplt.subplot(1, 5, 4)\nno_axis_show(canny_150200, title='Canny(150, 200)', cmap='gray')\n\ncanny_250300 = cv2.Canny(gray_img, 250, 300)\nplt.subplot(1, 5, 5)\nno_axis_show(canny_250300, title='Canny(250, 300)', cmap='gray')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\n \nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\n \nimport time\n\n\nsource_transform = transforms.Compose([\n    # Turn RGB to grayscale. (Bacause Canny do not support RGB images.)\n    transforms.Grayscale(),\n    # cv2 do not support skimage.Image, so we transform it to np.array, \n    # and then adopt cv2.Canny algorithm.\n    transforms.Lambda(lambda x: cv2.Canny(np.array(x), 170, 300)),\n    # Transform np.array back to the skimage.Image.\n    transforms.ToPILImage(),\n    # 50% Horizontal Flip. (For Augmentation)\n    transforms.RandomHorizontalFlip(),\n    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n    # if there's empty pixel after rotation.\n    transforms.RandomRotation(15, fill=(0,)),\n    # Transform to tensor for model inputs.\n    transforms.ToTensor(), \n])\ntarget_transform = transforms.Compose([\n    # Turn RGB to grayscale.\n    transforms.Grayscale(),\n    # Resize: size of source data is 32x32, thus we need to \n    #  enlarge the size of target data from 28x28 to 32x32。\n    transforms.Resize((32, 32)),\n    # 50% Horizontal Flip. (For Augmentation)\n    transforms.RandomHorizontalFlip(),\n    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n    # if there's empty pixel after rotation.\n    transforms.RandomRotation(15, fill=(0,)),\n    # Transform to tensor for model inputs.\n    transforms.ToTensor(),\n])\n \nsource_dataset = ImageFolder('real_or_drawing/train_data', transform=source_transform)\ntarget_dataset = ImageFolder('real_or_drawing/test_data', transform=target_transform)\n \nsource_dataloader = DataLoader(source_dataset, batch_size=32, shuffle=True)\ntarget_dataloader = DataLoader(target_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(target_dataset, batch_size=128, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureExtractor(nn.Module):\n\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 64, 3, 1, 1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(64, 128, 3, 1, 1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(128, 256, 3, 1, 1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(256, 256, 3, 1, 1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(256, 512, 3, 1, 1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        \n    def forward(self, x):\n        x = self.conv(x).squeeze()\n        return x\n\nclass LabelPredictor(nn.Module):\n\n    def __init__(self):\n        super(LabelPredictor, self).__init__()\n\n        self.layer = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.ReLU(),\n\n            nn.Linear(512, 512),\n            nn.ReLU(),\n\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, h):\n        c = self.layer(h)\n        return c\n\nclass DomainClassifier(nn.Module):\n\n    def __init__(self):\n        super(DomainClassifier, self).__init__()\n\n        self.layer = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n\n            nn.Linear(512, 1),\n        )\n\n    def forward(self, h):\n        y = self.layer(h)\n        return y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extractor = FeatureExtractor().cuda()\nlabel_predictor = LabelPredictor().cuda()\ndomain_classifier = DomainClassifier().cuda()\n\nclass_criterion = nn.CrossEntropyLoss()\ndomain_criterion = nn.BCEWithLogitsLoss()\n\noptimizer_F = optim.Adam(feature_extractor.parameters())\noptimizer_C = optim.Adam(label_predictor.parameters())\noptimizer_D = optim.Adam(domain_classifier.parameters())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\ndef train_epoch(source_dataloader, target_dataloader, lamb):\n    '''\n      Args:\n        source_dataloader: source data的dataloader\n        target_dataloader: target data的dataloader\n        lamb: control the balance of domain adaptatoin and classification.\n    '''\n\n    # D loss: Domain Classifier的loss\n    # F loss: Feature Extrator & Label Predictor的loss\n    running_D_loss, running_F_loss = 0.0, 0.0\n    total_hit, total_num = 0.0, 0.0\n    \n    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n\n        source_data = source_data.cuda()\n        source_label = source_label.cuda()\n        target_data = target_data.cuda()\n        \n        # Mixed the source data and target data, or it'll mislead the running params\n        #   of batch_norm. (runnning mean/var of soucre and target data are different.)\n        mixed_data = torch.cat([source_data, target_data], dim=0)\n        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n        # set domain label of source data to be 1.\n        domain_label[:source_data.shape[0]] = 1\n\n        # Step 1 : train domain classifier\n        feature = feature_extractor(mixed_data)\n        # We don't need to train feature extractor in step 1.\n        # Thus we detach the feature neuron to avoid backpropgation.\n        domain_logits = domain_classifier(feature.detach())\n        loss = domain_criterion(domain_logits, domain_label)\n        running_D_loss+= loss.item()\n        loss.backward()\n        optimizer_D.step()\n\n        # Step 2 : train feature extractor and label classifier\n        class_logits = label_predictor(feature[:source_data.shape[0]])\n        domain_logits = domain_classifier(feature)\n        # loss = cross entropy of classification - lamb * domain binary cross entropy.\n        #  The reason why using subtraction is similar to generator loss in disciminator of GAN\n        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n        running_F_loss+= loss.item()\n        loss.backward()\n        optimizer_F.step()\n        optimizer_C.step()\n\n        optimizer_D.zero_grad()\n        optimizer_F.zero_grad()\n        optimizer_C.zero_grad()\n\n        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n        total_num += source_data.shape[0]\n        \n    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num\n\nnum_epochs = 2000\n# train 200 epochs\n\n\nfor epoch in range(num_epochs):\n    lamb = np.log(1.02+1.7*epoch/num_epochs)\n    train_D_loss, train_F_loss, train_acc = train_epoch(source_dataloader, target_dataloader, lamb)\n            \n    if epoch == 10:\n        torch.save(feature_extractor.state_dict(), f'extractor_model_early.bin')\n        torch.save(label_predictor.state_dict(), f'predictor_model_early.bin')\n    elif epoch == 100:\n        torch.save(feature_extractor.state_dict(), f'extractor_model_mid.bin')\n        torch.save(label_predictor.state_dict(), f'predictor_model_mid.bin')\n          \n    torch.save(feature_extractor.state_dict(), f'extractor_model.bin')\n    torch.save(label_predictor.state_dict(), f'predictor_model.bin')\n    print('epoch {:>3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, acc {:6.4f}'.format(epoch, train_D_loss, train_F_loss, train_acc))\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extractor = FeatureExtractor().cuda()\nlabel_predictor = LabelPredictor().cuda()\n\nfeature_extractor.load_state_dict(torch.load(f'extractor_model.bin'))\nlabel_predictor.load_state_dict(torch.load(f'predictor_model.bin'))\n\nclass_criterion = nn.CrossEntropyLoss()\n\noptimizer = optim.Adam(list(feature_extractor.parameters()) + list(label_predictor.parameters()), lr=1e-3)\n\n\nt_feature_extractor = FeatureExtractor().cuda()\nt_label_predictor = LabelPredictor().cuda()\n\nt_feature_extractor.load_state_dict(torch.load(f'extractor_model.bin'))\nt_label_predictor.load_state_dict(torch.load(f'predictor_model.bin'))\nt_feature_extractor.eval()\nt_label_predictor.eval()\n\ndef new_state(model1, model2, beta=0.9):\n    sd1 = model1.state_dict()\n    sd2 = model2.state_dict()\n    for key in sd2:\n        sd2[key] = sd1[key] * (1 - beta) + sd2[key]*beta\n        \n    model2.load_state_dict(sd2)\n    model2.eval()\n\n\nfor i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n\n        source_data = source_data.cuda()\n        source_label = source_label.cuda()\n        target_data = target_data.cuda()\n        mixed_data = torch.cat([source_data, target_data], dim=0)\n        \n        class_logits = label_predictor(feature_extractor(target_data))\n        with torch.no_grad():\n            t_class_logits = t_label_predictor(t_feature_extractor(target_data))\n        # loss = cross entropy of classification - lamb * domain binary cross entropy.\n        #  The reason why using subtraction is similar to generator loss in disciminator of GAN\n        loss_s = class_criterion(class_logits[:source_data.shape[0]], source_label)\n        logits,t_logits=class_logits,t_class_logits\n        prob2, pseudo_label2 = logits.softmax(dim=1).max(dim=1)\n        prob, pseudo_label = t_logits.softmax(dim=1).max(dim=1)\n        print(prob2)\n        print(prob)\n        break\n\n\nce = nn.CrossEntropyLoss(reduction='none')\ndef c_loss(logits, t_logits):\n    prob2, pseudo_label2 = logits.softmax(dim=1).max(dim=1)\n    prob, pseudo_label = t_logits.softmax(dim=1).max(dim=1)\n    flag = prob > 0.95\n    return (flag * ce(logits, pseudo_label)).sum() / (flag.sum() + 1e-8), flag.sum(), torch.sum((pseudo_label==pseudo_label2) & flag).item()/(flag.sum().item() + 1e-8)\n\ndef train_epoch(source_dataloader, target_dataloader):\n    running_loss = 0.0\n    total_hit, total_num = 0.0, 0.0\n    total_t_used, total_t = 0.0, 0.0\n    pacc=0.0\n\n    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n\n        source_data = source_data.cuda()\n        source_label = source_label.cuda()\n        target_data = target_data.cuda()\n        mixed_data = torch.cat([source_data, target_data], dim=0)\n        \n        class_logits = label_predictor(feature_extractor(mixed_data))\n        with torch.no_grad():\n            t_class_logits = t_label_predictor(t_feature_extractor(target_data))\n        # loss = cross entropy of classification - lamb * domain binary cross entropy.\n        #  The reason why using subtraction is similar to generator loss in disciminator of GAN\n        loss_s = class_criterion(class_logits[:source_data.shape[0]], source_label)\n        loss_t, num, pa= c_loss(class_logits[source_data.shape[0]:], t_class_logits)\n        loss = loss_s + loss_t\n        running_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        pacc+=pa\n        total_t_used += num\n        total_t += target_data.shape[0]\n        total_hit += torch.sum(torch.argmax(class_logits[:source_data.shape[0]], dim=1) == source_label).item()\n        total_num += source_data.shape[0]\n        print(i, end='\\r') \n        \n    new_state(feature_extractor, t_feature_extractor)\n    new_state(label_predictor, t_label_predictor)\n        \n    return running_loss / (i+1), total_hit / total_num, total_t_used/total_t, pacc/(i+1)\n\n# train 200 epochs\nprint('start training')\nepochs = 800\ngap = 200\nmarked_epoch = [0] + [gap*i - 1 for i in range(1, epochs//gap + 1)]\nfor epoch in range(epochs):\n    train_loss, train_acc, used_rate,pred_acc = train_epoch(source_dataloader, target_dataloader)\n    if epoch in marked_epoch:\n        torch.save(feature_extractor.state_dict(), f'extractor_model_{epoch}.bin')\n        torch.save(label_predictor.state_dict(), f'predictor_model_{epoch}.bin')\n\n    print('epoch {:>3d}: train loss: {:6.4f}, acc: {:6.4f}, used rate {:6.4f},pred_acc {:6.4f}'.format(epoch, train_loss, train_acc, used_rate,pred_acc))\n\ntorch.save(feature_extractor.state_dict(), f'extractor_model.bin')\ntorch.save(label_predictor.state_dict(), f'predictor_model.bin')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = []\nlabel_predictor.eval()\nfeature_extractor.eval()\n\nfor i, (test_data, _) in (enumerate(test_dataloader)):\n    test_data = test_data.cuda()\n\n    class_logits = label_predictor(feature_extractor(test_data))\n\n    x = torch.argmax(class_logits, dim=1).cpu().detach().numpy()\n    result.append(x)\n\n\nimport pandas as pd\nresult = np.concatenate(result)\n\n# Generate your submission\ndf = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\ndf.to_csv('DaNN_submission.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import manifold\n\nclass Feature():\n    def __init__(self):\n        self.X = []\n        self.TX = []\n        self.labels = []\n\ndef get_features(model_list):\n    features = []\n    for model in model_list:\n        model.cuda()\n        model.eval()\n        features.append(Feature())\n    for (x, y), (tx, _) in zip(source_dataloader, target_dataloader):\n        x , tx= x.cuda(), tx.cuda()\n        for i, model in enumerate(model_list):\n            features[i].X.append(model(x).detach().cpu())\n            features[i].TX.append(model(tx).detach().cpu())\n            features[i].labels.append(y)\n    \n    for feature in features:\n        feature.X = torch.cat(feature.X).numpy()\n        feature.TX = torch.cat(feature.TX).numpy()\n        feature.labels = torch.cat(feature.labels).numpy()\n        \n    return features\n\ndef visualization(features):\n    for i, feature in enumerate(features):\n        data = np.concatenate([feature.X, feature.TX])\n        num_source = len(feature.labels)\n        X_tsne = manifold.TSNE(n_components=2, init='random', random_state=5, verbose=1).fit_transform(data)\n        # Normalization the processed features \n        x_min, x_max = X_tsne.min(0), X_tsne.max(0)\n        X_norm = (X_tsne - x_min) / (x_max - x_min)\n    \n        plt.figure(figsize=(16, 8))\n        plt.subplot(121)\n        plt.title(f'epoch {marked_epoch[i]}:distribution of features accross different class')\n        plt.scatter(X_norm[:num_source, 0], X_norm[:num_source, 1], c=feature.labels, label='source domain')\n        plt.subplot(122)\n        plt.title(f'epoch {marked_epoch[i]}:distribution of features accross different domain')\n        plt.scatter(X_norm[:num_source, 0], X_norm[:num_source, 1], c='b', label='source domain')\n        plt.scatter(X_norm[num_source:, 0], X_norm[num_source:, 1], c='r', label='target domain', alpha=0.5)\n        plt.legend()\n    plt.show()\n\n\nmodel_list = []\nfor epoch in marked_epoch:\n    model = FeatureExtractor()\n    model.load_state_dict(torch.load(f'extractor_model_{epoch}.bin'))\n    model_list.append(model)\n    \nvisualization(get_features(model_list))","metadata":{},"execution_count":null,"outputs":[]}]}