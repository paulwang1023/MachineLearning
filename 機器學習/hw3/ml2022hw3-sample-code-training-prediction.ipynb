{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":19.351342,"end_time":"2022-02-23T10:03:06.247288","exception":false,"start_time":"2022-02-23T10:02:46.895946","status":"completed"},"tags":[],"scrolled":true,"execution":{"iopub.status.busy":"2023-03-03T14:46:41.949498Z","iopub.execute_input":"2023-03-03T14:46:41.949924Z","iopub.status.idle":"2023-03-03T14:46:49.834568Z","shell.execute_reply.started":"2023-03-03T14:46:41.949883Z","shell.execute_reply":"2023-03-03T14:46:49.832988Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"_exp_name = \"sample\"","metadata":{"papermill":{"duration":0.0189,"end_time":"2022-02-23T10:03:06.279758","exception":false,"start_time":"2022-02-23T10:03:06.260858","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-03T14:46:49.836795Z","iopub.execute_input":"2023-03-03T14:46:49.837394Z","iopub.status.idle":"2023-03-03T14:46:49.842658Z","shell.execute_reply.started":"2023-03-03T14:46:49.837344Z","shell.execute_reply":"2023-03-03T14:46:49.841219Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Import necessary packages.\nimport numpy as np\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random","metadata":{"papermill":{"duration":1.654263,"end_time":"2022-02-23T10:03:07.947242","exception":false,"start_time":"2022-02-23T10:03:06.292979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-03T14:46:49.844938Z","iopub.execute_input":"2023-03-03T14:46:49.845615Z","iopub.status.idle":"2023-03-03T14:46:53.070017Z","shell.execute_reply.started":"2023-03-03T14:46:49.845559Z","shell.execute_reply":"2023-03-03T14:46:53.068661Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"myseed = 6666  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"papermill":{"duration":0.078771,"end_time":"2022-02-23T10:03:08.039428","exception":false,"start_time":"2022-02-23T10:03:07.960657","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-03T14:46:53.072918Z","iopub.execute_input":"2023-03-03T14:46:53.073456Z","iopub.status.idle":"2023-03-03T14:46:53.083090Z","shell.execute_reply.started":"2023-03-03T14:46:53.073418Z","shell.execute_reply":"2023-03-03T14:46:53.081654Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## **Transforms**\nTorchvision provides lots of useful utilities for image preprocessing, data wrapping as well as data augmentation.\n\nPlease refer to PyTorch official website for details about different transforms.","metadata":{"papermill":{"duration":0.01289,"end_time":"2022-02-23T10:03:08.065357","exception":false,"start_time":"2022-02-23T10:03:08.052467","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Normally, We don't need augmentations in testing and validation.\n# All we need here is to resize the PIL image and transform it into Tensor.\ntest_tfm = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# However, it is also possible to use augmentation in the testing phase.\n# You may use train_tfm to produce a variety of images and then test using ensemble methods\n#Augmentation A\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.Resize((224, 224)),\n    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.05),\n    transforms.RandomRotation(30),\n    transforms.RandomEqualize(0.1),\n    transforms.RandomPosterize(4,p=0.1),\n    transforms.RandomAdjustSharpness(4,p=0.1),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.CenterCrop(200),\n    transforms.AutoAugment(),\n    # ToTensor() should be the last one of the transforms.\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225)),\n])\n#Augmentation B\n'''\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.Resize((224, 224)),\n    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n    transforms.RandomGrayscale(p=0.2),\n    transforms.RandomPerspective(distortion_scale=0.5, p=0.3),\n    transforms.RandomInvert(0.2),\n    transforms.RandomRotation(40),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.AutoAugment(),\n    # You may add some transforms here.\n    # ToTensor() should be the last one of the transforms.\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225)),\n])\n'''\n#Augmentation C\n'''\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.Resize((224, 224)),\n    transforms.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.15),\n    transforms.RandomGrayscale(p=0.2),\n    transforms.RandomPerspective(distortion_scale=0.6, p=0.3),\n    transforms.RandomInvert(0.2),\n    transforms.RandomRotation(45),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.AutoAugment(),\n    # You may add some transforms here.\n    # ToTensor() should be the last one of the transforms.\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225)),\n])\n'''\n","metadata":{"papermill":{"duration":0.021406,"end_time":"2022-02-23T10:03:08.099437","exception":false,"start_time":"2022-02-23T10:03:08.078031","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-03T14:46:53.086692Z","iopub.execute_input":"2023-03-03T14:46:53.087129Z","iopub.status.idle":"2023-03-03T14:46:53.103174Z","shell.execute_reply.started":"2023-03-03T14:46:53.087087Z","shell.execute_reply":"2023-03-03T14:46:53.101568Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'\\ntrain_tfm = transforms.Compose([\\n    # Resize the image into a fixed shape (height = width = 128)\\n    transforms.Resize((224, 224)),\\n    transforms.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.15),\\n    transforms.RandomGrayscale(p=0.2),\\n    transforms.RandomPerspective(distortion_scale=0.6, p=0.3),\\n    transforms.RandomInvert(0.2),\\n    transforms.RandomRotation(45),\\n    transforms.RandomHorizontalFlip(p=0.5),\\n    transforms.RandomVerticalFlip(p=0.5),\\n    transforms.AutoAugment(),\\n    # You may add some transforms here.\\n    # ToTensor() should be the last one of the transforms.\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225)),\\n])\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":" LabelSmoothing cited from\n https://towardsdatascience.com/label-smoothing-as-another-regularization-trick-7b34c50dc0b9\n \n \n and\nhttps://gist.githubusercontent.com/dpoulopoulos/96166605a92a867edd3205c6dea1213a/raw/4d8ab6b2085ff4ddf8792e37b8686de1700bf65a/label_smoothing_1.py\n and\n https://gist.githubusercontent.com/dpoulopoulos/64148f96cb073e7ddac33d3a877879ab/raw/e81e5aea3efd05ae942b06d1b50b8d73938249ea/label_smoothing_2.py","metadata":{}},{"cell_type":"code","source":"def linear_combination(x, y, epsilon): \n    return epsilon*x + (1-epsilon)*y\nimport torch.nn.functional as F\n\n\ndef reduce_loss(loss, reduction='mean'):\n    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n\n\nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, epsilon:float=0.1, reduction='mean'):\n        super().__init__()\n        self.epsilon = epsilon\n        self.reduction = reduction\n    \n    def forward(self, preds, target):\n        n = preds.size()[-1]\n        log_preds = F.log_softmax(preds, dim=-1)\n        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n        return linear_combination(loss/n, nll, self.epsilon)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T14:46:53.105176Z","iopub.execute_input":"2023-03-03T14:46:53.105689Z","iopub.status.idle":"2023-03-03T14:46:53.119083Z","shell.execute_reply.started":"2023-03-03T14:46:53.105639Z","shell.execute_reply":"2023-03-03T14:46:53.117983Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## **Datasets**\nThe data is labelled by the name, so we load images and label while calling '__getitem__'","metadata":{"papermill":{"duration":0.012739,"end_time":"2022-02-23T10:03:08.125181","exception":false,"start_time":"2022-02-23T10:03:08.112442","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class FoodDataset(Dataset):\n\n    def __init__(self,path,tfm=test_tfm,files = None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n        print(f\"One {path} sample\",self.files[0])\n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        im = self.transform(im)\n        #im = self.data[idx]\n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n        return im,label\n\n","metadata":{"papermill":{"duration":0.023022,"end_time":"2022-02-23T10:03:08.160912","exception":false,"start_time":"2022-02-23T10:03:08.13789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-03T14:46:53.120627Z","iopub.execute_input":"2023-03-03T14:46:53.120996Z","iopub.status.idle":"2023-03-03T14:46:53.134700Z","shell.execute_reply.started":"2023-03-03T14:46:53.120932Z","shell.execute_reply":"2023-03-03T14:46:53.133636Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\n_dataset_dir = \"../input/ml2022spring-hw3b/food11\"\n# Construct datasets.\n# The argument \"loader\" tells how torchvision reads the data.\ntrain_set = FoodDataset(os.path.join(_dataset_dir,\"training\"), tfm=train_tfm)\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\nvalid_set = FoodDataset(os.path.join(_dataset_dir,\"validation\"), tfm=test_tfm)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)","metadata":{"papermill":{"duration":0.054295,"end_time":"2022-02-23T10:03:08.266338","exception":false,"start_time":"2022-02-23T10:03:08.212043","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-03T14:46:53.136735Z","iopub.execute_input":"2023-03-03T14:46:53.137124Z","iopub.status.idle":"2023-03-03T14:46:53.182618Z","shell.execute_reply.started":"2023-03-03T14:46:53.137085Z","shell.execute_reply":"2023-03-03T14:46:53.181048Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"One ../input/ml2022spring-hw3b/food11/training sample ../input/ml2022spring-hw3b/food11/training/0_0.jpg\nOne ../input/ml2022spring-hw3b/food11/validation sample ../input/ml2022spring-hw3b/food11/validation/0_0.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"# \"cuda\" only when GPUs are available.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# The number of training epochs and patience.\nn_epochs = 1\npatience = 300 # If no improvement in 'patience' epochs, early stop\n\n# Initialize a model, and put it on the device specified.\nmodel = models.resnext50_32x4d(pretrained = False).to(device)\n# After training for 12 , continue training\n'''\n_your_dataset_name = \"../input/model18\"\nmodel.load_state_dict(torch.load(f\"{_your_dataset_name}/{_exp_name}_best.ckpt\"))\n'''\n# For the classification task, we use cross-entropy as the measurement of performance.\ncriterion = nn.CrossEntropyLoss()\n# loss for training the 3rd time\n'''criterion1 = LabelSmoothingCrossEntropy()'''\n# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n#lr=0.0001 for training the 2nd and 3rd time\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5) \n\n# Initialize trackers, these are not parameters and should not be changed\nstale = 0\nbest_acc = 0\n\nfor epoch in range(n_epochs):\n\n    # ---------- Training ----------\n    # Make sure the model is in train mode before training.\n    model.train()\n\n    # These are used to record information in training.\n    train_loss = []\n    train_accs = []\n\n    for batch in tqdm(train_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n        #imgs = imgs.half()\n        #print(imgs.shape,labels.shape)\n\n        # Forward the data. (Make sure data and model are on the same device.)\n        logits = model(imgs.to(device))\n\n        # Calculate the cross-entropy loss.\n        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n        loss = criterion(logits, labels.to(device))\n        #loss for training the 3rd time\n        '''loss = criterion1(logits, labels.to(device))'''\n        # Gradients stored in the parameters in the previous step should be cleared out first.\n        optimizer.zero_grad()\n\n        # Compute the gradients for parameters.\n        loss.backward()\n\n        # Clip the gradient norms for stable training.\n        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n\n        # Update the parameters with computed gradients.\n        optimizer.step()\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        train_loss.append(loss.item())\n        train_accs.append(acc)\n        \n    train_loss = sum(train_loss) / len(train_loss)\n    train_acc = sum(train_accs) / len(train_accs)\n\n    # Print the information.\n    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n\n    # ---------- Validation ----------\n    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n    model.eval()\n\n    # These are used to record information in validation.\n    valid_loss = []\n    valid_accs = []\n\n    # Iterate the validation set by batches.\n    for batch in tqdm(valid_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n        #imgs = imgs.half()\n\n        # We don't need gradient in validation.\n        # Using torch.no_grad() accelerates the forward process.\n        with torch.no_grad():\n            logits = model(imgs.to(device))\n\n        # We can still compute the loss (but not the gradient).\n        loss = criterion(logits, labels.to(device))\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        valid_loss.append(loss.item())\n        valid_accs.append(acc)\n        #break\n\n    # The average loss and accuracy for entire validation set is the average of the recorded values.\n    valid_loss = sum(valid_loss) / len(valid_loss)\n    valid_acc = sum(valid_accs) / len(valid_accs)\n\n    # Print the information.\n    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n    # update logs\n    if valid_acc > best_acc:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n    else:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n    # save models\n    if valid_acc > best_acc:\n        print(f\"Best model found at epoch {epoch}, saving model\")\n        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n        best_acc = valid_acc\n        stale = 0\n    else:\n        stale += 1\n        if stale > patience:\n            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n            break","metadata":{"papermill":{"duration":32830.720158,"end_time":"2022-02-23T19:10:19.001001","exception":false,"start_time":"2022-02-23T10:03:08.280843","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-03T14:46:53.184599Z","iopub.execute_input":"2023-03-03T14:46:53.185083Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9b1881c2aeb40108845db250b833a4f"}},"metadata":{}}]}]}