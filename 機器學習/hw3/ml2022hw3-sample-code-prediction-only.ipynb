{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-20T17:58:42.150313Z","iopub.execute_input":"2022-03-20T17:58:42.150687Z","iopub.status.idle":"2022-03-20T17:58:46.699409Z","shell.execute_reply.started":"2022-03-20T17:58:42.150554Z","shell.execute_reply":"2022-03-20T17:58:46.698585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This code is for generating predictions only\n\n## For this code to work as intended, please make sure the following:\n\n1. _exp_name is the same as your training code\n2. Classifier is the same as you training code\n3. Your trained model is save as a dataset and loaded with + Add data (See slides for detail)\n4. model.load_state_dict({path}) is the correct path to the model","metadata":{}},{"cell_type":"code","source":"_exp_name = \"sample\"","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:58:46.701028Z","iopub.execute_input":"2022-03-20T17:58:46.701295Z","iopub.status.idle":"2022-03-20T17:58:46.705817Z","shell.execute_reply.started":"2022-03-20T17:58:46.701258Z","shell.execute_reply":"2022-03-20T17:58:46.704953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary packages.\nimport numpy as np\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random\nfrom datetime import datetime\nnow = str(datetime.now())","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:58:46.707013Z","iopub.execute_input":"2022-03-20T17:58:46.707782Z","iopub.status.idle":"2022-03-20T17:58:48.398543Z","shell.execute_reply.started":"2022-03-20T17:58:46.707742Z","shell.execute_reply":"2022-03-20T17:58:48.397749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myseed = 6666  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:58:48.400217Z","iopub.execute_input":"2022-03-20T17:58:48.400496Z","iopub.status.idle":"2022-03-20T17:58:48.453525Z","shell.execute_reply.started":"2022-03-20T17:58:48.400449Z","shell.execute_reply":"2022-03-20T17:58:48.452584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normally, We don't need augmentations in testing and validation.\n# All we need here is to resize the PIL image and transform it into Tensor.\ntest_tfm = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# However, it is also possible to use augmentation in the testing phase.\n# You may use train_tfm to produce a variety of images and then test using ensemble methods\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.Resize((224, 224)),\n    # You may add some transforms here.\n    # ToTensor() should be the last one of the transforms.\n    transforms.ToTensor(),\n])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:58:48.457493Z","iopub.execute_input":"2022-03-20T17:58:48.458090Z","iopub.status.idle":"2022-03-20T17:58:48.462926Z","shell.execute_reply.started":"2022-03-20T17:58:48.458037Z","shell.execute_reply":"2022-03-20T17:58:48.462030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FoodDataset(Dataset):\n\n    def __init__(self,path,tfm=test_tfm,files = None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n        print(f\"One {path} sample\",self.files[0])\n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        im = self.transform(im)\n        #im = self.data[idx]\n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n        return im,label","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:58:48.464329Z","iopub.execute_input":"2022-03-20T17:58:48.465003Z","iopub.status.idle":"2022-03-20T17:58:48.477661Z","shell.execute_reply.started":"2022-03-20T17:58:48.464966Z","shell.execute_reply":"2022-03-20T17:58:48.476785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loader","metadata":{}},{"cell_type":"code","source":"batch_size = 64\n_dataset_dir = \"../input/ml2022spring-hw3b/food11\"\ntest_set = FoodDataset(os.path.join(_dataset_dir,\"test\"), tfm=test_tfm)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:58:48.479245Z","iopub.execute_input":"2022-03-20T17:58:48.479602Z","iopub.status.idle":"2022-03-20T17:58:48.500453Z","shell.execute_reply.started":"2022-03-20T17:58:48.479489Z","shell.execute_reply":"2022-03-20T17:58:48.499740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_your_dataset_name = \"../input/model18-2\" \n_your_dataset_name1 = \"../input/model18-3\"\n_your_dataset_name2 = \"../input/model18\"\nif not len(_your_dataset_name):\n    print(_your_dataset_name)\n    assert (\"Default name detected, please fill in the name of your dataset(model checkpoint)\")\n# \"cuda\" only when GPUs are available.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_best = models.resnext50_32x4d(pretrained = False).to(device)\nmodel_best.load_state_dict(torch.load(f\"{_your_dataset_name}/{_exp_name}_best.ckpt\"))\nmodel_best.eval()\nmodel_best1 = models.resnext50_32x4d(pretrained = False).to(device)\nmodel_best1.load_state_dict(torch.load(f\"{_your_dataset_name1}/{_exp_name}_best.ckpt\"))\nmodel_best1.eval()\nmodel_best2 = models.resnext50_32x4d(pretrained = False).to(device)\nmodel_best2.load_state_dict(torch.load(f\"{_your_dataset_name2}/{_exp_name}_best.ckpt\"))\nmodel_best2.eval()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:58:48.501913Z","iopub.execute_input":"2022-03-20T17:58:48.502211Z","iopub.status.idle":"2022-03-20T17:58:54.747604Z","shell.execute_reply.started":"2022-03-20T17:58:48.502177Z","shell.execute_reply":"2022-03-20T17:58:54.746912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict\n\n#### Modify this part to do test time augmentation","metadata":{}},{"cell_type":"code","source":"prediction = []\nwith torch.no_grad():\n    for data,_ in test_loader:\n        test_pred = model_best(data.to(device))+model_best1(data.to(device))+model_best2(data.to(device))\n        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n        prediction += test_label.squeeze().tolist()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:58:54.748656Z","iopub.execute_input":"2022-03-20T17:58:54.749485Z","iopub.status.idle":"2022-03-20T17:59:54.143006Z","shell.execute_reply.started":"2022-03-20T17:58:54.749446Z","shell.execute_reply":"2022-03-20T17:59:54.142270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create test csv\ndef pad4(i):\n    return \"0\"*(4-len(str(i)))+str(i)\ndf = pd.DataFrame()\ndf[\"Id\"] = [pad4(i) for i in range(1,len(test_set)+1)]\ndf[\"Category\"] = prediction\ndf.to_csv(\"submission.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:59:54.144135Z","iopub.execute_input":"2022-03-20T17:59:54.145144Z","iopub.status.idle":"2022-03-20T17:59:54.177549Z","shell.execute_reply.started":"2022-03-20T17:59:54.145108Z","shell.execute_reply":"2022-03-20T17:59:54.176933Z"},"trusted":true},"execution_count":null,"outputs":[]}]}