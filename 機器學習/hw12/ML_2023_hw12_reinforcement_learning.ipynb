{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9cd40bf342b043a98373ee5c6e2d5c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b24989bd8ca433c858b90db0ac0b6cd",
              "IPY_MODEL_d642eb47526a4066938396fbe32d6652",
              "IPY_MODEL_45678e717d304ddc9ddbfaa991878dfb"
            ],
            "layout": "IPY_MODEL_112402d58210494eb3022798dcbb2195"
          }
        },
        "3b24989bd8ca433c858b90db0ac0b6cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_236fcd0a9dc84c5b842f7488699a41df",
            "placeholder": "​",
            "style": "IPY_MODEL_ac93771f238a412b92523f2b4582c1eb",
            "value": "Total:  102.6, Final:  20.5:  24%"
          }
        },
        "d642eb47526a4066938396fbe32d6652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f35e0fa0590943308a552c0f0725a100",
            "max": 600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b23540749c3048b384e2bd3130329857",
            "value": 143
          }
        },
        "45678e717d304ddc9ddbfaa991878dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea936d4baabe4783a67d2399dee047ea",
            "placeholder": "​",
            "style": "IPY_MODEL_8df9d11e12aa49cbad9d2f3c70813eb0",
            "value": " 143/600 [34:06&lt;2:32:15, 19.99s/it]"
          }
        },
        "112402d58210494eb3022798dcbb2195": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "236fcd0a9dc84c5b842f7488699a41df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac93771f238a412b92523f2b4582c1eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f35e0fa0590943308a552c0f0725a100": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b23540749c3048b384e2bd3130329857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea936d4baabe4783a67d2399dee047ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8df9d11e12aa49cbad9d2f3c70813eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp30SB4bxeQb"
      },
      "source": [
        "# **Homework 12 - Reinforcement Learning**\n",
        "\n",
        "If you have any problem, e-mail us at mlta-2023-spring@googlegroups.com\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXsnCWPtWSNk"
      },
      "source": [
        "## Preliminary work\n",
        "\n",
        "First, we need to install all necessary packages.\n",
        "One of them, gym, builded by OpenAI, is a toolkit for developing Reinforcement Learning algorithm. Other packages are for visualization in colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e2bScpnkVbv",
        "outputId": "85d42f3c-d25a-4d08-969d-ea704a6cec6d"
      },
      "source": [
        "!apt update\n",
        "!apt install python-opengl xvfb -y\n",
        "!pip install -q swig\n",
        "!pip install box2d==2.3.2 gym[box2d]==0.25.2 box2d-py pyvirtualdisplay tqdm numpy==1.22.4\n",
        "!pip install box2d==2.3.2 box2d-kengz\n",
        "!pip freeze > requirements.txt\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "21 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-2build1).\n",
            "xvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: box2d==2.3.2 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: gym[box2d]==0.25.2 in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.10/dist-packages (2.3.5)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: numpy==1.22.4 in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.25.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.25.2) (0.0.8)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.25.2) (2.1.0)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.25.2) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: box2d==2.3.2 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: box2d-kengz in /usr/local/lib/python3.10/dist-packages (2.3.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_-i3cdoYsks"
      },
      "source": [
        "\n",
        "Next, set up virtual display，and import all necessaary packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl2nREINDLiw"
      },
      "source": [
        "%%capture\n",
        "from pyvirtualdisplay import Display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaEJ8BUCpN9P"
      },
      "source": [
        "# Warning ! Do not revise random seed !!!\n",
        "# Your submission on JudgeBoi will not reproduce your result !!!\n",
        "Make your HW result to be reproducible.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV9i8i2YkRbO",
        "outputId": "49eb92d3-878e-4464-f4de-a230f71a9c7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seed = 2023 # Do not change this\n",
        "def fix(env, seed):\n",
        "  env.seed(seed)\n",
        "  env.action_space.seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He0XDx6bzjgC"
      },
      "source": [
        "Last, call gym and build an [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_4-xJcbBt09"
      },
      "source": [
        "%%capture\n",
        "import gym\n",
        "import random\n",
        "env = gym.make('LunarLander-v2')\n",
        "fix(env, seed) # fix the environment Do not revise this !!!"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrkVvTrvWZ5H"
      },
      "source": [
        "## What Lunar Lander？\n",
        "\n",
        "“LunarLander-v2”is to simulate the situation when the craft lands on the surface of the moon.\n",
        "\n",
        "This task is to enable the craft to land \"safely\" at the pad between the two yellow flags.\n",
        "> Landing pad is always at coordinates (0,0).\n",
        "> Coordinates are the first two numbers in state vector.\n",
        "\n",
        "![](https://gym.openai.com/assets/docs/aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e53.svg)\n",
        "\n",
        "\"LunarLander-v2\" actually includes \"Agent\" and \"Environment\".\n",
        "\n",
        "In this homework, we will utilize the function `step()` to control the action of \"Agent\".\n",
        "\n",
        "Then `step()` will return the observation/state and reward given by the \"Environment\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIbp82sljvAt"
      },
      "source": [
        "### Observation / State\n",
        "\n",
        "First, we can take a look at what an Observation / State looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsXZra3N9R5T",
        "outputId": "792ac21b-ca53-4874-a828-f342e9ed0a95"
      },
      "source": [
        "print(env.observation_space)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
            " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
            " 1.       ], (8,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezdfoThbAQ49"
      },
      "source": [
        "\n",
        "`Box(8,)`means that observation is an 8-dim vector\n",
        "### Action\n",
        "\n",
        "Actions can be taken by looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1k4dIrBAaKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ccf8f5b-cc5a-448e-e6aa-c05eb83b2837"
      },
      "source": [
        "print(env.action_space)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dejXT6PHBrPn"
      },
      "source": [
        "`Discrete(4)` implies that there are four kinds of actions can be taken by agent.\n",
        "- 0 implies the agent will not take any actions\n",
        "- 2 implies the agent will accelerate downward\n",
        "- 1, 3 implies the agent will accelerate left and right\n",
        "\n",
        "Next, we will try to make the agent interact with the environment.\n",
        "Before taking any actions, we recommend to call `reset()` function to reset the environment. Also, this function will return the initial state of the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi4OmrmZgnWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628fd41d-335b-4ace-ae1a-d2dc1da51420"
      },
      "source": [
        "initial_state = env.reset()\n",
        "print(initial_state)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.00506535  1.413064   -0.5130838   0.09527162  0.00587628  0.11622101\n",
            "  0.          0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBx0mEqqgxJ9"
      },
      "source": [
        "Then, we try to get a random action from the agent's action space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxkOEXRKgizt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b292fd-533f-405e-9ff0-f89512a6df71"
      },
      "source": [
        "random_action = env.action_space.sample()\n",
        "print(random_action)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mns-bO01g0-J"
      },
      "source": [
        "More, we can utilize `step()` to make agent act according to the randomly-selected `random_action`.\n",
        "The `step()` function will return four values:\n",
        "- observation / state\n",
        "- reward\n",
        "- done (True/ False)\n",
        "- Other information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_WViSxGgIk9"
      },
      "source": [
        "observation, reward, done, info = env.step(random_action)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK7r126kuCNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fcd9b3d-51f3-4989-c7be-138cb9cb7e9f"
      },
      "source": [
        "print(done)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKdS8vOihxhc"
      },
      "source": [
        "### Reward\n",
        "\n",
        "\n",
        "> Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxQNs77hi0_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ec64d5-87e2-46d8-c40a-64042db2fd1c"
      },
      "source": [
        "print(reward)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.4981841929643156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhqp6D-XgHpe"
      },
      "source": [
        "### Random Agent\n",
        "In the end, before we start training, we can see whether a random agent can successfully land the moon or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3G0bxoccelv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "7881daab-1009-4d8e-c6db-c7e740ae2104"
      },
      "source": [
        "env.reset()\n",
        "\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, _ = env.step(action)\n",
        "\n",
        "    img.set_data(env.render(mode='rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5paWqo7tWL2"
      },
      "source": [
        "## Policy Gradient\n",
        "Now, we can build a simple policy network. The network will return one of action in the action space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8tdmeD-tZew"
      },
      "source": [
        "class PolicyGradientNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(8, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 4)\n",
        "\n",
        "    def forward(self, state):\n",
        "        hid = torch.tanh(self.fc1(state))\n",
        "        hid = torch.tanh(hid)\n",
        "        return F.softmax(self.fc3(hid), dim=-1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynbqJrhIFTC3"
      },
      "source": [
        "Then, we need to build a simple agent. The agent will acts according to the output of the policy network above. There are a few things can be done by agent:\n",
        "- `learn()`：update the policy network from log probabilities and rewards.\n",
        "- `sample()`：After receiving observation from the environment, utilize policy network to tell which action to take. The return values of this function includes action and log probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZo-IxJx286z",
        "outputId": "67ca2ac5-e990-4caf-ed24-b719f9f4920a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "class ReplayMemory:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "    def __init__(self, CAPACITY):\n",
        "        self.capacity = CAPACITY\n",
        "        self.memory = []\n",
        "        self.index = 0\n",
        "        self.transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "    def push(self, state, action, state_next, reward):\n",
        "        \"\"\"Push a new experience to memory.\"\"\"\n",
        "        if len(self.memory) < self.capacity: # if still has capacity, initialize memory[index] to None\n",
        "            self.memory.append(None)\n",
        "\n",
        "        self.memory[self.index] = self.transition(state, action, state_next, reward)\n",
        "\n",
        "        self.index = (self.index + 1) % self.capacity  # circular index\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size=8, action_size=4, fc1_units=64, fc2_units=64):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "class DQNAgent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        \"\"\"Initialize an Agent object.\"\"\"\n",
        "\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory_capacity = 10000\n",
        "        self.memory = ReplayMemory(self.memory_capacity)\n",
        "\n",
        "        # Q-Network\n",
        "        self.main_q_network = DQN()\n",
        "        self.target_q_network = DQN()\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = optim.RMSprop(self.main_q_network.parameters(), lr=1e-4)\n",
        "\n",
        "    def update_q_function(self):\n",
        "        '''update q function'''\n",
        "\n",
        "        # no enough samples, just return\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        # If enough samples are available in memory, get random subset and learn\n",
        "        self.batch, self.state_batch, self.action_batch, self.reward_batch, self.non_final_next_states = self.make_minibatch()\n",
        "\n",
        "        self.expected_state_action_values = self.get_expected_state_action_values()\n",
        "\n",
        "        self.update_main_q_network()\n",
        "\n",
        "    def make_minibatch(self):\n",
        "        '''Creating a mini-batch'''\n",
        "\n",
        "        transitions = self.memory.sample(BATCH_SIZE)\n",
        "\n",
        "        Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                           if s is not None])\n",
        "\n",
        "        return batch, state_batch, action_batch, reward_batch, non_final_next_states\n",
        "\n",
        "    def get_expected_state_action_values(self):\n",
        "        '''calculate Q（St,at）'''\n",
        "\n",
        "        self.main_q_network.eval()\n",
        "        self.target_q_network.eval()\n",
        "\n",
        "        self.state_action_values = self.main_q_network(\n",
        "            self.state_batch).gather(1, self.action_batch)\n",
        "\n",
        "        non_final_mask = torch.BoolTensor(tuple(map(lambda s: s is not None,\n",
        "                                                    self.batch.next_state)))\n",
        "        # set all state to 0\n",
        "        next_state_values = torch.zeros(BATCH_SIZE)\n",
        "\n",
        "        next_state_values[non_final_mask] = self.target_q_network(\n",
        "            self.non_final_next_states).max(1)[0].detach()\n",
        "        # DQN formula\n",
        "        expected_state_action_values = self.reward_batch + GAMMA * next_state_values\n",
        "\n",
        "        return expected_state_action_values\n",
        "\n",
        "    def get_action(self, state, episode, test=False):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        if test:\n",
        "            self.main_q_network.eval()\n",
        "            with torch.no_grad():\n",
        "                # t.max(1) will return largest column value of each row.\n",
        "                # second column on max result is index of where max element was\n",
        "                # found, so we pick action with the larger expected reward.\n",
        "                action = self.main_q_network(torch.from_numpy(state).unsqueeze(0)).max(1)[1].view(1, 1)\n",
        "            return action.item()\n",
        "\n",
        "        global steps_done\n",
        "        # Epsilon-greedy policy\n",
        "        #epsilon = episode\n",
        "        #epsilon = 0.5 * (1 / (episode + 1))\n",
        "        epsilon = EPS_END + (EPS_START - EPS_END) * \\\n",
        "                np.exp(-1. * steps_done / EPS_DECAY)\n",
        "        #print('epsilon', epsilon)\n",
        "\n",
        "        steps_done += 1\n",
        "\n",
        "        if epsilon <= np.random.uniform(0, 1):\n",
        "            #print('use max')\n",
        "            self.main_q_network.eval()\n",
        "            with torch.no_grad():\n",
        "                # t.max(1) will return largest column value of each row.\n",
        "                # second column on max result is index of where max element was\n",
        "                # found, so we pick action with the larger expected reward.\n",
        "                action = self.main_q_network(state).max(1)[1].view(1, 1)\n",
        "        else:\n",
        "            #print('random')\n",
        "            action = torch.LongTensor(\n",
        "                [[random.randrange(self.num_actions)]])\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update_main_q_network(self):\n",
        "\n",
        "        '''update main q net'''\n",
        "\n",
        "        # set train mode\n",
        "        self.main_q_network.train()\n",
        "        # Hurberloss function\n",
        "        # expected_state_action_values (minbatch,)->(minbatchx1)\n",
        "\n",
        "        loss = F.smooth_l1_loss(self.state_action_values,\n",
        "                                self.expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "        # update\n",
        "        self.optimizer.zero_grad()  # reset gradient\n",
        "        loss.backward()  # backpropagation\n",
        "        for param in self.main_q_network.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        self.optimizer.step()  # update network\n",
        "\n",
        "\n",
        "    def memorize(self, state, action, state_next, reward):\n",
        "        '''save state, action, state_next, reward into replay memory'''\n",
        "        self.memory.push(state, action, state_next, reward)\n",
        "\n",
        "    def update_target_q_function(self):\n",
        "\n",
        "        '''synchronize Target Q-Network to Main Q-Network'''\n",
        "        self.target_q_network.load_state_dict(self.main_q_network.state_dict())\n",
        "'''\n",
        "class Action(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(8, 64)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, 4)\n",
        "\n",
        "  def forward(self, state):\n",
        "      hid = torch.tanh(self.fc1(state))\n",
        "      hid = torch.tanh(self.fc2(hid))\n",
        "      return F.softmax(self.fc3(hid), dim=-1)\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(8, 64)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, 4)\n",
        "\n",
        "  def forward(self, state):\n",
        "      hid = torch.tanh(self.fc1(state))\n",
        "      hid = torch.tanh(self.fc2(hid))\n",
        "      return F.softmax(self.fc3(hid), dim=-1)\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "class DQN():\n",
        "    def __init__(self,action,critic):\n",
        "        self.action_net = action\n",
        "        self.critic_net = critic\n",
        "        self.optimizer = optim.Adam(self.action_net.parameters(), lr=5e-4)\n",
        "        self.critic_net.load_state_dict(self.action_net.state_dict()) # 加载action_net的行为\n",
        "        self.critic_net.eval() # 模型验证\n",
        "    def forward(self, state):\n",
        "        return self.action_net(state)\n",
        "    def learn(self, state_action_values, expected_state_action_values,batch):\n",
        "        loss = torch.zeros(1)\n",
        "        for i in range(len(state_action_values)):\n",
        "            loss += F.smooth_l1_loss(state_action_values[i], expected_state_action_values[i])\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        # 每10个batch更新一次target_net\n",
        "        if batch%10 == 9:\n",
        "            self.critic_net.load_state_dict(self.action_net.state_dict())\n",
        "    def sample(self, state):\n",
        "        r = torch.randn(1)\n",
        "        if r > 0.2:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                return self.action_net(state).argmax().item()\n",
        "        else:\n",
        "            return torch.tensor([[random.randrange(4)]], device=device, dtype=torch.long).item()\n",
        "'''\n",
        "'''\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "class PolicyGradientAgent():\n",
        "\n",
        "    def __init__(self, network):\n",
        "        self.network = network\n",
        "        self.optimizer = optim.SGD(self.network.parameters(), lr=0.002)\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.network(state)\n",
        "    def learn(self, log_probs, rewards):\n",
        "        loss = (-log_probs * rewards).sum() # You don't need to revise this to pass simple baseline (but you can)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def sample(self, state):\n",
        "        action_prob = self.network(torch.FloatTensor(state))\n",
        "        action_dist = Categorical(action_prob)\n",
        "        action = action_dist.sample()\n",
        "        log_prob = action_dist.log_prob(action)\n",
        "        return action.item(), log_prob\n",
        "'''"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom torch.optim.lr_scheduler import StepLR\\nclass PolicyGradientAgent():\\n    \\n    def __init__(self, network):\\n        self.network = network\\n        self.optimizer = optim.SGD(self.network.parameters(), lr=0.002)\\n        \\n    def forward(self, state):\\n        return self.network(state)\\n    def learn(self, log_probs, rewards):\\n        loss = (-log_probs * rewards).sum() # You don't need to revise this to pass simple baseline (but you can)\\n\\n        self.optimizer.zero_grad()\\n        loss.backward()\\n        self.optimizer.step()\\n        \\n    def sample(self, state):\\n        action_prob = self.network(torch.FloatTensor(state))\\n        action_dist = Categorical(action_prob)\\n        action = action_dist.sample()\\n        log_prob = action_dist.log_prob(action)\\n        return action.item(), log_prob\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehPlnTKyRZf9"
      },
      "source": [
        "Lastly, build a network and agent to start training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfJIvML-RYjL"
      },
      "source": [
        "#network = PolicyGradientNetwork()\n",
        "#agent = PolicyGradientAgent(network)\n",
        "#network = DQN()\n",
        "agent = DQNAgent(num_states=8,num_actions=4)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouv23glgf5Qt"
      },
      "source": [
        "## Training Agent\n",
        "\n",
        "Now let's start to train our agent.\n",
        "Through taking all the interactions between agent and environment as training data, the policy network can learn from all these attempts,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg5rxBBaf38_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9cd40bf342b043a98373ee5c6e2d5c05",
            "3b24989bd8ca433c858b90db0ac0b6cd",
            "d642eb47526a4066938396fbe32d6652",
            "45678e717d304ddc9ddbfaa991878dfb",
            "112402d58210494eb3022798dcbb2195",
            "236fcd0a9dc84c5b842f7488699a41df",
            "ac93771f238a412b92523f2b4582c1eb",
            "f35e0fa0590943308a552c0f0725a100",
            "b23540749c3048b384e2bd3130329857",
            "ea936d4baabe4783a67d2399dee047ea",
            "8df9d11e12aa49cbad9d2f3c70813eb0"
          ]
        },
        "outputId": "afc818df-ae6e-4026-80a8-1dc4a48306b7"
      },
      "source": [
        "EPISODE_PER_BATCH = 5  # 每蒐集 5 個 episodes 更新一次 agent\n",
        "NUM_BATCH = 600        # 總共更新 400 次\n",
        "BATCH_SIZE = 32\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "\n",
        "best_score = 0\n",
        "best_batch = 0\n",
        "agent.main_q_network.train()\n",
        "agent.target_q_network.train()\n",
        "steps_done = 0\n",
        "\n",
        "avg_total_rewards, avg_final_rewards = [], []\n",
        "\n",
        "prg_bar = tqdm(range(NUM_BATCH))\n",
        "for batch in prg_bar:\n",
        "\n",
        "    rewards = []\n",
        "    total_rewards, final_rewards = [], []\n",
        "\n",
        "    # 蒐集訓練資料\n",
        "    for episode in range(EPISODE_PER_BATCH):\n",
        "\n",
        "        observation = env.reset() # 環境的初始化\n",
        "        state = observation  # 將觀測結果直接當成狀態s使用\n",
        "        state = torch.from_numpy(state).type(\n",
        "                torch.FloatTensor)  # 將NumPy變數轉換成PyTorch的張量\n",
        "        state = torch.unsqueeze(state, 0)  # 將size 4轉換成size 1x4\n",
        "        total_reward, total_step = 0, 0\n",
        "\n",
        "        while True:\n",
        "\n",
        "            action = agent.get_action(state, batch)  # 求出動作\n",
        "            # 執行動作a_t後，算出s_{t+1}與done旗標\n",
        "            # 根據action指定.item()、再取得內容\n",
        "            observation_next, reward, done, _ = env.step(action.item())  # 不會用到info，所以設定為_\n",
        "\n",
        "            total_reward += reward\n",
        "            total_step += 1\n",
        "            rewards.append(reward) #改這裡\n",
        "            # ! 重要 ！\n",
        "            # 現在的reward 的implementation 為每個時刻的瞬時reward, 給定action_list : a1, a2, a3 ......\n",
        "            #                                                       reward :     r1, r2 ,r3 ......\n",
        "            # medium：將reward調整成accumulative decaying reward, 給定action_list : a1,                         a2,                           a3 ......\n",
        "            #                                                       reward :     r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,r3+0.99*r4+0.99^2*r5+ ......\n",
        "            # boss : implement DQN\n",
        "            if done:\n",
        "                state_next = None  # 沒有下個狀態，所以存入None\n",
        "\n",
        "            else:\n",
        "                state_next = observation_next  # 直接將觀測結果當成狀態使用\n",
        "                state_next = torch.from_numpy(state_next).type(\n",
        "                        torch.FloatTensor)  # 將numpy變數轉換成PyTorch的張量\n",
        "                state_next = torch.unsqueeze(state_next, 0)  # 將size 4轉換成size 1x4\n",
        "\n",
        "            # 將學習經驗存入記憶體\n",
        "            agent.memorize(state, action, state_next, torch.FloatTensor([reward]))\n",
        "\n",
        "            # 以Experience Replay更新Q函數\n",
        "            agent.update_q_function()\n",
        "\n",
        "            # 觀測狀態的更新\n",
        "            state = state_next\n",
        "\n",
        "            # 結束時的處理\n",
        "            if done:\n",
        "                final_rewards.append(reward)\n",
        "                total_rewards.append(total_reward)\n",
        "                break\n",
        "\n",
        "\n",
        "    #print(f\"rewards looks like \", np.shape(rewards))\n",
        "    # 紀錄訓練過程\n",
        "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
        "    avg_total_rewards.append(avg_total_reward)\n",
        "    avg_final_rewards.append(avg_final_reward)\n",
        "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
        "\n",
        "    # 更新網路\n",
        "    # rewards = np.concatenate(rewards, axis=0)\n",
        "    #rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # 將 reward 正規標準化\n",
        "    agent.update_target_q_function()\n",
        "    print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(np.array(rewards)).size())\n",
        "\n",
        "\n",
        "    ### testing\n",
        "\n",
        "    fix(env, seed)\n",
        "    agent.main_q_network.eval()  # 測試前先將 network 切換為 evaluation 模式\n",
        "    NUM_OF_TEST = 5 # Do not revise it !!!!!\n",
        "    test_total_reward = []\n",
        "    action_list = []\n",
        "    for i in range(NUM_OF_TEST):\n",
        "        actions = []\n",
        "        state = env.reset()\n",
        "\n",
        "        #img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "        total_reward = 0\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.get_action(state, episode=i, test=True)\n",
        "            actions.append(action)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            #img.set_data(env.render(mode='rgb_array'))\n",
        "            #display.display(plt.gcf())\n",
        "            #display.clear_output(wait=True)\n",
        "        print(total_reward)\n",
        "        test_total_reward.append(total_reward)\n",
        "\n",
        "        action_list.append(actions) #儲存你測試的結果\n",
        "        print(\"length of actions is \", len(actions))\n",
        "    print(f\"Your final reward is : %.2f\"%np.mean(test_total_reward))\n",
        "    if np.mean(test_total_reward) > 250:\n",
        "        distribution = {}\n",
        "        for actions in action_list:\n",
        "            for action in actions:\n",
        "                if action not in distribution.keys():\n",
        "                    distribution[action] = 1\n",
        "                else:\n",
        "                    distribution[action] += 1\n",
        "        PATH = \"Action_List_test\" + str(batch) + \".npy\" # 可以改成你想取的名字或路徑\n",
        "        np.save(PATH ,np.array(action_list))\n",
        "        if np.mean(test_total_reward) > best_score:\n",
        "            best_score = np.mean(test_total_reward)\n",
        "            best_batch = batch\n",
        "            print('Improve to score %.2f at batch %d'% (best_score, best_batch ))\n",
        "'''\n",
        "agent.action_net.train()  # 訓練前，先確保 network 處在 training 模式\n",
        "EPISODE_PER_BATCH = 5  # 每蒐集 5 個 episodes 更新一次 agent\n",
        "NUM_BATCH = 600        # 總共更新 400 次\n",
        "avg_total_rewards, avg_final_rewards = [], []\n",
        "prg_bar = tqdm(range(NUM_BATCH))\n",
        "for batch in prg_bar:\n",
        "    state_action_values, expected_state_action_values = [], []\n",
        "    total_rewards, final_rewards = [], []\n",
        "    # 收集训练资料\n",
        "    for episode in range(EPISODE_PER_BATCH):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            action = agent.sample(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            state_action_value = agent.action_net(torch.FloatTensor(state).to(device))[action]\n",
        "            state_action_values.append(state_action_value)\n",
        "            state = next_state\n",
        "            #Double DQN\n",
        "            if done:\n",
        "                next_state_value = 0\n",
        "            else:\n",
        "                next_state_value = agent.critic_net(torch.FloatTensor(state).to(device))[agent.action_net(torch.FloatTensor(state).to(device)).argmax().item()].detach()\n",
        "            # 使用Policy Gradient\n",
        "            expected_state_action_value = (next_state_value * 0.99) + reward\n",
        "            expected_state_action_values.append(torch.FloatTensor([expected_state_action_value]))\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                final_rewards.append(reward)\n",
        "                total_rewards.append(total_reward)\n",
        "                break\n",
        "\n",
        "    # 紀錄訓練過程\n",
        "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
        "    avg_total_rewards.append(avg_total_reward)\n",
        "    avg_final_rewards.append(avg_final_reward)\n",
        "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
        "    # print(expected_state_action_values)\n",
        "    # 更新網路\n",
        "    agent.learn(state_action_values, expected_state_action_values, batch)\n",
        "\n",
        "'''\n",
        "'''\n",
        "agent.network.train()  # Switch network into training mode\n",
        "EPISODE_PER_BATCH = 5  # update the  agent every 5 episode\n",
        "NUM_BATCH = 500        # totally update the agent for 400 time\n",
        "\n",
        "avg_total_rewards, avg_final_rewards = [], []\n",
        "\n",
        "prg_bar = tqdm(range(NUM_BATCH))\n",
        "for batch in prg_bar:\n",
        "\n",
        "    log_probs, rewards = [], []\n",
        "    total_rewards, final_rewards = [], []\n",
        "\n",
        "    # collect trajectory\n",
        "    for episode in range(EPISODE_PER_BATCH):\n",
        "\n",
        "        state = env.reset()\n",
        "        total_reward, total_step = 0, 0\n",
        "        seq_rewards = []\n",
        "        while True:\n",
        "\n",
        "            action, log_prob = agent.sample(state) # at, log(at|st)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
        "            # seq_rewards.append(reward)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            total_step += 1\n",
        "            rewards.append(reward) # change here\n",
        "            n_reward = reward\n",
        "            for i in range(len(rewards)-1):\n",
        "              n_reward *= 0.99\n",
        "              rewards[len(rewards)-2-i] += n_reward\n",
        "            if done:\n",
        "                final_rewards.append(reward)\n",
        "                total_rewards.append(total_reward)\n",
        "\n",
        "                break\n",
        "\n",
        "    print(f\"rewards looks like \", np.shape(rewards))\n",
        "    print(f\"log_probs looks like \", np.shape(log_probs))\n",
        "    # record training process\n",
        "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
        "    avg_total_rewards.append(avg_total_reward)\n",
        "    avg_final_rewards.append(avg_final_reward)\n",
        "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
        "\n",
        "    # update agent\n",
        "    # rewards = np.concatenate(rewards, axis=0)\n",
        "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # normalize the reward\n",
        "    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
        "    print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
        "    print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())\n",
        "\n",
        "'''\n",
        "'''\n",
        "agent.network.train()  # Switch network into training mode\n",
        "EPISODE_PER_BATCH = 5  # update the  agent every 5 episode\n",
        "NUM_BATCH = 100        # totally update the agent for 500 time\n",
        "\n",
        "avg_total_rewards, avg_final_rewards = [], []\n",
        "\n",
        "prg_bar = tqdm(range(NUM_BATCH))\n",
        "for batch in prg_bar:\n",
        "\n",
        "    log_probs, rewards = [], []\n",
        "    total_rewards, final_rewards = [], []\n",
        "\n",
        "    # collect trajectory\n",
        "    for episode in range(EPISODE_PER_BATCH):\n",
        "\n",
        "        state = env.reset()\n",
        "        total_reward, total_step = 0, 0\n",
        "        seq_rewards = []\n",
        "        while True:\n",
        "\n",
        "            action, log_prob = agent.sample(state) # at, log(at|st)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
        "            # seq_rewards.append(reward)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            total_step += 1\n",
        "            rewards.append(reward) # change here\n",
        "            # ! IMPORTANT !\n",
        "            # Current reward implementation: immediate reward,  given action_list : a1, a2, a3 ......\n",
        "            #                                                         rewards :     r1, r2 ,r3 ......\n",
        "            # medium：change \"rewards\" to accumulative decaying reward, given action_list : a1,                           a2,                           a3, ......\n",
        "            #                                                           rewards :           r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,  r3+0.99*r4+0.99^2*r5+ ......\n",
        "            # boss : implement Actor-Critic\n",
        "            if done:\n",
        "                final_rewards.append(reward)\n",
        "                total_rewards.append(total_reward)\n",
        "\n",
        "                break\n",
        "\n",
        "    print(f\"rewards looks like \", np.shape(rewards))\n",
        "    #print(f\"log_probs looks like \", np.shape(log_probs))\n",
        "    # record training process\n",
        "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
        "    avg_total_rewards.append(avg_total_reward)\n",
        "    avg_final_rewards.append(avg_final_reward)\n",
        "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
        "\n",
        "    # update agent\n",
        "    # rewards = np.concatenate(rewards, axis=0)\n",
        "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # normalize the reward\n",
        "    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
        "    print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
        "    print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/600 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cd40bf342b043a98373ee5c6e2d5c05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.from_numpy(rewards) looks like  torch.Size([400])\n",
            "-169.24737767540185\n",
            "length of actions is  76\n",
            "-97.80035892867915\n",
            "length of actions is  84\n",
            "-108.21350771248544\n",
            "length of actions is  79\n",
            "-114.88630094034575\n",
            "length of actions is  76\n",
            "-152.81054267812647\n",
            "length of actions is  65\n",
            "Your final reward is : -128.59\n",
            "torch.from_numpy(rewards) looks like  torch.Size([404])\n",
            "-132.63278859406427\n",
            "length of actions is  75\n",
            "-229.09042071196592\n",
            "length of actions is  76\n",
            "-128.03044869662298\n",
            "length of actions is  67\n",
            "-219.0283692430635\n",
            "length of actions is  71\n",
            "-276.1757812058139\n",
            "length of actions is  85\n",
            "Your final reward is : -196.99\n",
            "torch.from_numpy(rewards) looks like  torch.Size([354])\n",
            "-143.89890076653535\n",
            "length of actions is  75\n",
            "-200.4439344759164\n",
            "length of actions is  77\n",
            "-218.03534750920863\n",
            "length of actions is  93\n",
            "-165.0825493357254\n",
            "length of actions is  61\n",
            "-136.19168681846264\n",
            "length of actions is  60\n",
            "Your final reward is : -172.73\n",
            "torch.from_numpy(rewards) looks like  torch.Size([377])\n",
            "-165.89831643558477\n",
            "length of actions is  75\n",
            "-188.39865581355\n",
            "length of actions is  77\n",
            "-200.45105256534532\n",
            "length of actions is  94\n",
            "-148.8008350352126\n",
            "length of actions is  69\n",
            "-169.8680996193035\n",
            "length of actions is  64\n",
            "Your final reward is : -174.68\n",
            "torch.from_numpy(rewards) looks like  torch.Size([364])\n",
            "-123.52103230538913\n",
            "length of actions is  82\n",
            "-182.6273260926024\n",
            "length of actions is  61\n",
            "-145.4493109734005\n",
            "length of actions is  62\n",
            "-141.30847936021203\n",
            "length of actions is  74\n",
            "-128.9041780070525\n",
            "length of actions is  81\n",
            "Your final reward is : -144.36\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2353])\n",
            "-122.45953761602325\n",
            "length of actions is  1000\n",
            "-161.36996084053408\n",
            "length of actions is  1000\n",
            "-144.73290477686473\n",
            "length of actions is  1000\n",
            "-126.0407040647225\n",
            "length of actions is  1000\n",
            "-127.68659379760356\n",
            "length of actions is  1000\n",
            "Your final reward is : -136.46\n",
            "torch.from_numpy(rewards) looks like  torch.Size([5000])\n",
            "-160.09367615999784\n",
            "length of actions is  1000\n",
            "-151.42874619275966\n",
            "length of actions is  1000\n",
            "-155.887065560343\n",
            "length of actions is  1000\n",
            "-109.18574386746866\n",
            "length of actions is  1000\n",
            "-82.62368846921974\n",
            "length of actions is  1000\n",
            "Your final reward is : -131.84\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3071])\n",
            "-646.0651356141859\n",
            "length of actions is  298\n",
            "-542.01667632485\n",
            "length of actions is  344\n",
            "-285.17486089937915\n",
            "length of actions is  182\n",
            "-342.07181076083236\n",
            "length of actions is  461\n",
            "-579.180990059986\n",
            "length of actions is  231\n",
            "Your final reward is : -478.90\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1489])\n",
            "-121.9215983698796\n",
            "length of actions is  246\n",
            "-242.59798448522022\n",
            "length of actions is  241\n",
            "-125.1654440446895\n",
            "length of actions is  220\n",
            "-205.21748658627212\n",
            "length of actions is  289\n",
            "-272.8568721871085\n",
            "length of actions is  197\n",
            "Your final reward is : -193.55\n",
            "torch.from_numpy(rewards) looks like  torch.Size([980])\n",
            "-353.1978705470274\n",
            "length of actions is  121\n",
            "-83.49162499615122\n",
            "length of actions is  186\n",
            "-415.1038272818628\n",
            "length of actions is  172\n",
            "-331.228070540274\n",
            "length of actions is  184\n",
            "-18.789194921735145\n",
            "length of actions is  176\n",
            "Your final reward is : -240.36\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1039])\n",
            "-197.2229997962196\n",
            "length of actions is  128\n",
            "-366.4089842741818\n",
            "length of actions is  220\n",
            "-183.4978430073068\n",
            "length of actions is  149\n",
            "-231.39419118930795\n",
            "length of actions is  209\n",
            "-305.1798498674326\n",
            "length of actions is  237\n",
            "Your final reward is : -256.74\n",
            "torch.from_numpy(rewards) looks like  torch.Size([608])\n",
            "-323.8399580100805\n",
            "length of actions is  122\n",
            "-259.91990217963644\n",
            "length of actions is  121\n",
            "-399.3337853771427\n",
            "length of actions is  275\n",
            "-214.94673492762405\n",
            "length of actions is  170\n",
            "-246.79415070637967\n",
            "length of actions is  123\n",
            "Your final reward is : -288.97\n",
            "torch.from_numpy(rewards) looks like  torch.Size([919])\n",
            "-97.75198903346455\n",
            "length of actions is  149\n",
            "-255.80523928598032\n",
            "length of actions is  177\n",
            "-214.05102609598998\n",
            "length of actions is  201\n",
            "-273.64486469476583\n",
            "length of actions is  168\n",
            "-253.40976205391303\n",
            "length of actions is  124\n",
            "Your final reward is : -218.93\n",
            "torch.from_numpy(rewards) looks like  torch.Size([992])\n",
            "-332.213308305339\n",
            "length of actions is  251\n",
            "-343.7364169996529\n",
            "length of actions is  362\n",
            "-267.654679564646\n",
            "length of actions is  265\n",
            "-124.49975151496344\n",
            "length of actions is  254\n",
            "-73.14521540285988\n",
            "length of actions is  160\n",
            "Your final reward is : -228.25\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1153])\n",
            "-319.1153475782041\n",
            "length of actions is  255\n",
            "-350.90475538840553\n",
            "length of actions is  251\n",
            "-272.7621355770717\n",
            "length of actions is  187\n",
            "-223.55902010379305\n",
            "length of actions is  190\n",
            "-222.30202984580833\n",
            "length of actions is  353\n",
            "Your final reward is : -277.73\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1300])\n",
            "-328.794676203835\n",
            "length of actions is  262\n",
            "-309.8464333062535\n",
            "length of actions is  249\n",
            "-251.82814233687122\n",
            "length of actions is  298\n",
            "-222.65263803756326\n",
            "length of actions is  219\n",
            "-296.2656406769369\n",
            "length of actions is  252\n",
            "Your final reward is : -281.88\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1985])\n",
            "-210.72266748069723\n",
            "length of actions is  266\n",
            "-179.2691321759617\n",
            "length of actions is  1000\n",
            "11.636351679865342\n",
            "length of actions is  192\n",
            "-175.1254608861484\n",
            "length of actions is  1000\n",
            "-153.29697768460298\n",
            "length of actions is  1000\n",
            "Your final reward is : -141.36\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2031])\n",
            "-255.65210505056382\n",
            "length of actions is  260\n",
            "-130.16342580925442\n",
            "length of actions is  195\n",
            "-136.52685870163089\n",
            "length of actions is  1000\n",
            "-154.99403770416365\n",
            "length of actions is  1000\n",
            "-210.23775561209254\n",
            "length of actions is  245\n",
            "Your final reward is : -177.51\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2404])\n",
            "-152.49157051581324\n",
            "length of actions is  261\n",
            "-138.53941591042454\n",
            "length of actions is  1000\n",
            "-204.50094339522946\n",
            "length of actions is  169\n",
            "-105.77778569958743\n",
            "length of actions is  139\n",
            "-164.2191911679603\n",
            "length of actions is  1000\n",
            "Your final reward is : -153.11\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3037])\n",
            "-316.2159491537272\n",
            "length of actions is  492\n",
            "-142.3605695111546\n",
            "length of actions is  1000\n",
            "-154.63441168954702\n",
            "length of actions is  1000\n",
            "-150.10174141877548\n",
            "length of actions is  1000\n",
            "-94.66054683662159\n",
            "length of actions is  1000\n",
            "Your final reward is : -171.59\n",
            "torch.from_numpy(rewards) looks like  torch.Size([5000])\n",
            "-119.0770459247836\n",
            "length of actions is  1000\n",
            "-155.96051335424593\n",
            "length of actions is  1000\n",
            "-146.97008371445602\n",
            "length of actions is  1000\n",
            "-90.91300420742971\n",
            "length of actions is  1000\n",
            "-21.50383769574997\n",
            "length of actions is  1000\n",
            "Your final reward is : -106.88\n",
            "torch.from_numpy(rewards) looks like  torch.Size([5000])\n",
            "-107.71883801114592\n",
            "length of actions is  1000\n",
            "-155.27952697284533\n",
            "length of actions is  1000\n",
            "-135.87360176716845\n",
            "length of actions is  1000\n",
            "-51.779488357733044\n",
            "length of actions is  1000\n",
            "-49.93095963139851\n",
            "length of actions is  1000\n",
            "Your final reward is : -100.12\n",
            "torch.from_numpy(rewards) looks like  torch.Size([5000])\n",
            "-104.27872298753066\n",
            "length of actions is  1000\n",
            "-151.24429481732014\n",
            "length of actions is  1000\n",
            "-131.0238578963108\n",
            "length of actions is  1000\n",
            "-84.6845904468807\n",
            "length of actions is  1000\n",
            "-80.5146467097921\n",
            "length of actions is  1000\n",
            "Your final reward is : -110.35\n",
            "torch.from_numpy(rewards) looks like  torch.Size([5000])\n",
            "-76.13929026177554\n",
            "length of actions is  1000\n",
            "-123.19574198363398\n",
            "length of actions is  1000\n",
            "-112.50684733087044\n",
            "length of actions is  1000\n",
            "-69.0102225270087\n",
            "length of actions is  1000\n",
            "-64.34123617996435\n",
            "length of actions is  1000\n",
            "Your final reward is : -89.04\n",
            "torch.from_numpy(rewards) looks like  torch.Size([5000])\n",
            "-82.29504151136793\n",
            "length of actions is  1000\n",
            "-133.00141285543486\n",
            "length of actions is  1000\n",
            "-85.57702875489774\n",
            "length of actions is  1000\n",
            "-78.30316121847811\n",
            "length of actions is  1000\n",
            "-79.13385070103297\n",
            "length of actions is  1000\n",
            "Your final reward is : -91.66\n",
            "torch.from_numpy(rewards) looks like  torch.Size([5000])\n",
            "-183.01900494000267\n",
            "length of actions is  74\n",
            "-65.89711663121288\n",
            "length of actions is  1000\n",
            "-116.29577183812796\n",
            "length of actions is  61\n",
            "-163.12079359641393\n",
            "length of actions is  71\n",
            "-203.36928812898674\n",
            "length of actions is  80\n",
            "Your final reward is : -146.34\n",
            "torch.from_numpy(rewards) looks like  torch.Size([4713])\n",
            "-61.7237128095352\n",
            "length of actions is  1000\n",
            "-82.49958983767799\n",
            "length of actions is  1000\n",
            "-64.13161190347452\n",
            "length of actions is  1000\n",
            "-20.192441313219664\n",
            "length of actions is  1000\n",
            "-44.11704802264485\n",
            "length of actions is  1000\n",
            "Your final reward is : -54.53\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3641])\n",
            "96.21309349446422\n",
            "length of actions is  616\n",
            "-56.89995581406692\n",
            "length of actions is  1000\n",
            "155.3755896226591\n",
            "length of actions is  574\n",
            "-61.5812974821211\n",
            "length of actions is  409\n",
            "109.17018805386888\n",
            "length of actions is  920\n",
            "Your final reward is : 48.46\n",
            "torch.from_numpy(rewards) looks like  torch.Size([4083])\n",
            "64.52193556795805\n",
            "length of actions is  627\n",
            "-90.57257503154267\n",
            "length of actions is  154\n",
            "170.31595204790915\n",
            "length of actions is  722\n",
            "-44.98788810611606\n",
            "length of actions is  269\n",
            "-111.02904144417592\n",
            "length of actions is  300\n",
            "Your final reward is : -2.35\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3568])\n",
            "85.34030097210966\n",
            "length of actions is  481\n",
            "122.70561987334916\n",
            "length of actions is  692\n",
            "-98.00505226553067\n",
            "length of actions is  373\n",
            "-96.92753198054092\n",
            "length of actions is  385\n",
            "139.66188449006364\n",
            "length of actions is  508\n",
            "Your final reward is : 30.56\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3315])\n",
            "84.67397831980962\n",
            "length of actions is  459\n",
            "171.66840355919322\n",
            "length of actions is  539\n",
            "-23.459530522826643\n",
            "length of actions is  1000\n",
            "153.13020341306833\n",
            "length of actions is  390\n",
            "125.39983738386454\n",
            "length of actions is  720\n",
            "Your final reward is : 102.28\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2703])\n",
            "-95.9760789838869\n",
            "length of actions is  296\n",
            "-72.86752059756371\n",
            "length of actions is  443\n",
            "-122.26134580483115\n",
            "length of actions is  255\n",
            "181.16985892057843\n",
            "length of actions is  965\n",
            "-81.01291527944812\n",
            "length of actions is  200\n",
            "Your final reward is : -38.19\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3000])\n",
            "-108.55287833327445\n",
            "length of actions is  275\n",
            "-40.17627367266279\n",
            "length of actions is  314\n",
            "-112.26704445599013\n",
            "length of actions is  245\n",
            "-42.47746011076202\n",
            "length of actions is  1000\n",
            "-11.241412833668988\n",
            "length of actions is  1000\n",
            "Your final reward is : -62.94\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2325])\n",
            "115.33457493111581\n",
            "length of actions is  481\n",
            "153.53737091685835\n",
            "length of actions is  550\n",
            "-43.61005584374738\n",
            "length of actions is  1000\n",
            "-93.18181906821204\n",
            "length of actions is  277\n",
            "-27.8897423353456\n",
            "length of actions is  1000\n",
            "Your final reward is : 20.84\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3048])\n",
            "160.5525606412011\n",
            "length of actions is  477\n",
            "-83.45251823368973\n",
            "length of actions is  705\n",
            "-40.89421684588774\n",
            "length of actions is  1000\n",
            "-79.16474943069873\n",
            "length of actions is  210\n",
            "100.93807068332978\n",
            "length of actions is  976\n",
            "Your final reward is : 11.60\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2421])\n",
            "-125.01045176976127\n",
            "length of actions is  280\n",
            "-116.20550503296575\n",
            "length of actions is  1000\n",
            "-75.50137849801965\n",
            "length of actions is  172\n",
            "-52.05333135179914\n",
            "length of actions is  122\n",
            "160.2724383921927\n",
            "length of actions is  666\n",
            "Your final reward is : -41.70\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2332])\n",
            "-133.51953956774895\n",
            "length of actions is  309\n",
            "-74.27412719085436\n",
            "length of actions is  148\n",
            "-99.21731388803452\n",
            "length of actions is  621\n",
            "109.86888946781308\n",
            "length of actions is  681\n",
            "137.9600536407017\n",
            "length of actions is  554\n",
            "Your final reward is : -11.84\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2855])\n",
            "-97.85019957454375\n",
            "length of actions is  365\n",
            "177.3761423660937\n",
            "length of actions is  577\n",
            "209.78488652405565\n",
            "length of actions is  638\n",
            "-77.68837412443105\n",
            "length of actions is  446\n",
            "86.64494274668857\n",
            "length of actions is  973\n",
            "Your final reward is : 59.65\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2436])\n",
            "-107.80212924848163\n",
            "length of actions is  394\n",
            "-54.822968799336664\n",
            "length of actions is  168\n",
            "-83.3515749491162\n",
            "length of actions is  1000\n",
            "-50.92818051230424\n",
            "length of actions is  397\n",
            "-80.73126879518671\n",
            "length of actions is  197\n",
            "Your final reward is : -75.53\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3498])\n",
            "129.3965277388233\n",
            "length of actions is  469\n",
            "-126.40464753873809\n",
            "length of actions is  572\n",
            "181.28007051746295\n",
            "length of actions is  500\n",
            "-116.76751594942448\n",
            "length of actions is  649\n",
            "-161.69927857766808\n",
            "length of actions is  684\n",
            "Your final reward is : -18.84\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2556])\n",
            "-106.75248792183592\n",
            "length of actions is  386\n",
            "162.67308342450892\n",
            "length of actions is  878\n",
            "203.41030095223698\n",
            "length of actions is  531\n",
            "148.46858263083772\n",
            "length of actions is  640\n",
            "167.4090984110919\n",
            "length of actions is  992\n",
            "Your final reward is : 115.04\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1958])\n",
            "-93.25982562784947\n",
            "length of actions is  376\n",
            "-52.028961677424995\n",
            "length of actions is  386\n",
            "50.83615170850166\n",
            "length of actions is  857\n",
            "211.59478166015626\n",
            "length of actions is  425\n",
            "-49.96495514325835\n",
            "length of actions is  150\n",
            "Your final reward is : 13.44\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2174])\n",
            "-102.61527956839757\n",
            "length of actions is  366\n",
            "-45.642695097102376\n",
            "length of actions is  187\n",
            "-45.98662865189263\n",
            "length of actions is  199\n",
            "150.7135538501979\n",
            "length of actions is  490\n",
            "213.65267425916653\n",
            "length of actions is  495\n",
            "Your final reward is : 34.02\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1763])\n",
            "-96.0476378783437\n",
            "length of actions is  408\n",
            "186.4629845235109\n",
            "length of actions is  386\n",
            "217.93995010004772\n",
            "length of actions is  480\n",
            "178.67160131946878\n",
            "length of actions is  531\n",
            "203.70285174473435\n",
            "length of actions is  510\n",
            "Your final reward is : 138.15\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2135])\n",
            "-59.472381807893456\n",
            "length of actions is  391\n",
            "-72.93267033389203\n",
            "length of actions is  444\n",
            "186.7189181027065\n",
            "length of actions is  480\n",
            "178.28522509138566\n",
            "length of actions is  553\n",
            "196.34583950359428\n",
            "length of actions is  502\n",
            "Your final reward is : 85.79\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2611])\n",
            "-88.15244316980926\n",
            "length of actions is  401\n",
            "-66.72683440574917\n",
            "length of actions is  333\n",
            "-87.6400212037435\n",
            "length of actions is  251\n",
            "-82.83211090211726\n",
            "length of actions is  170\n",
            "-86.99351298366481\n",
            "length of actions is  646\n",
            "Your final reward is : -82.47\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3001])\n",
            "119.84169639805883\n",
            "length of actions is  658\n",
            "-125.35907832372848\n",
            "length of actions is  223\n",
            "163.25976574607733\n",
            "length of actions is  613\n",
            "90.940243397791\n",
            "length of actions is  1000\n",
            "203.90663314364252\n",
            "length of actions is  482\n",
            "Your final reward is : 90.52\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1507])\n",
            "-80.64700907443824\n",
            "length of actions is  374\n",
            "210.71519036223043\n",
            "length of actions is  522\n",
            "125.10508711405676\n",
            "length of actions is  537\n",
            "-39.67473499932423\n",
            "length of actions is  294\n",
            "187.94496984152084\n",
            "length of actions is  280\n",
            "Your final reward is : 80.69\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1426])\n",
            "-98.69571966860869\n",
            "length of actions is  408\n",
            "-20.99644766268611\n",
            "length of actions is  204\n",
            "127.66265037185246\n",
            "length of actions is  674\n",
            "-50.49195988190776\n",
            "length of actions is  360\n",
            "211.28798060644465\n",
            "length of actions is  427\n",
            "Your final reward is : 33.75\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1791])\n",
            "-86.87896828923985\n",
            "length of actions is  417\n",
            "-97.25590722638512\n",
            "length of actions is  228\n",
            "207.67797303992273\n",
            "length of actions is  589\n",
            "18.97797363477798\n",
            "length of actions is  1000\n",
            "121.36155831255564\n",
            "length of actions is  695\n",
            "Your final reward is : 32.78\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2019])\n",
            "117.10400014203852\n",
            "length of actions is  714\n",
            "-51.14542275131656\n",
            "length of actions is  453\n",
            "-166.28391315731471\n",
            "length of actions is  674\n",
            "-65.53796367542498\n",
            "length of actions is  151\n",
            "-80.22760302176493\n",
            "length of actions is  147\n",
            "Your final reward is : -49.22\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2007])\n",
            "102.20827222177645\n",
            "length of actions is  702\n",
            "-48.94294439715807\n",
            "length of actions is  450\n",
            "-101.10305025590179\n",
            "length of actions is  173\n",
            "-75.88462376829244\n",
            "length of actions is  136\n",
            "77.26012818700971\n",
            "length of actions is  895\n",
            "Your final reward is : -9.29\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1843])\n",
            "105.07972033377524\n",
            "length of actions is  808\n",
            "95.84693446642645\n",
            "length of actions is  525\n",
            "-96.97769406306625\n",
            "length of actions is  122\n",
            "-94.8053204296259\n",
            "length of actions is  382\n",
            "140.5457363075405\n",
            "length of actions is  739\n",
            "Your final reward is : 29.94\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2583])\n",
            "-84.64174510026011\n",
            "length of actions is  430\n",
            "215.1643149371833\n",
            "length of actions is  464\n",
            "209.140587012271\n",
            "length of actions is  398\n",
            "-50.02316077282224\n",
            "length of actions is  214\n",
            "-68.4772093775096\n",
            "length of actions is  355\n",
            "Your final reward is : 44.23\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1058])\n",
            "61.38073897098981\n",
            "length of actions is  973\n",
            "-26.26249059656923\n",
            "length of actions is  1000\n",
            "192.56048748170502\n",
            "length of actions is  461\n",
            "128.93916080488606\n",
            "length of actions is  912\n",
            "-63.84255812801742\n",
            "length of actions is  196\n",
            "Your final reward is : 58.56\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2556])\n",
            "96.43113592300817\n",
            "length of actions is  846\n",
            "221.76031503962426\n",
            "length of actions is  419\n",
            "-153.10339802552969\n",
            "length of actions is  218\n",
            "190.22711718081803\n",
            "length of actions is  294\n",
            "119.76387668343386\n",
            "length of actions is  676\n",
            "Your final reward is : 95.02\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2115])\n",
            "75.54496569633403\n",
            "length of actions is  928\n",
            "-96.6319374355284\n",
            "length of actions is  273\n",
            "-79.7699846397111\n",
            "length of actions is  588\n",
            "97.54706561812837\n",
            "length of actions is  500\n",
            "-101.6841141963548\n",
            "length of actions is  136\n",
            "Your final reward is : -21.00\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2014])\n",
            "-105.35865281968155\n",
            "length of actions is  1000\n",
            "119.94913520904414\n",
            "length of actions is  633\n",
            "129.71110738556462\n",
            "length of actions is  648\n",
            "-60.25333718695465\n",
            "length of actions is  285\n",
            "-99.46265279610881\n",
            "length of actions is  345\n",
            "Your final reward is : -3.08\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2552])\n",
            "90.30402865492229\n",
            "length of actions is  852\n",
            "-108.42775005162862\n",
            "length of actions is  409\n",
            "-59.79375756581746\n",
            "length of actions is  589\n",
            "-4.160854893277758\n",
            "length of actions is  245\n",
            "-68.87566828228887\n",
            "length of actions is  164\n",
            "Your final reward is : -30.19\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2233])\n",
            "108.43503907243552\n",
            "length of actions is  826\n",
            "-104.5607419941542\n",
            "length of actions is  142\n",
            "-142.7365488595022\n",
            "length of actions is  505\n",
            "151.10244048632637\n",
            "length of actions is  478\n",
            "161.5846660963852\n",
            "length of actions is  687\n",
            "Your final reward is : 34.76\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2580])\n",
            "-40.46831015076443\n",
            "length of actions is  1000\n",
            "102.98715073796056\n",
            "length of actions is  626\n",
            "195.87459636253325\n",
            "length of actions is  515\n",
            "-34.34556993136244\n",
            "length of actions is  1000\n",
            "-128.5138240742812\n",
            "length of actions is  537\n",
            "Your final reward is : 19.11\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3670])\n",
            "-34.15643941479851\n",
            "length of actions is  1000\n",
            "-116.62635589907916\n",
            "length of actions is  343\n",
            "-33.44296532893971\n",
            "length of actions is  1000\n",
            "-24.728605664534225\n",
            "length of actions is  1000\n",
            "106.09644508369219\n",
            "length of actions is  677\n",
            "Your final reward is : -20.57\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2940])\n",
            "-28.645926270170833\n",
            "length of actions is  1000\n",
            "-79.81912962402251\n",
            "length of actions is  208\n",
            "-5.560784881298439\n",
            "length of actions is  1000\n",
            "-52.611360214017154\n",
            "length of actions is  305\n",
            "-73.18912146563369\n",
            "length of actions is  145\n",
            "Your final reward is : -47.97\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2687])\n",
            "-21.668510233476606\n",
            "length of actions is  1000\n",
            "149.18034299141857\n",
            "length of actions is  436\n",
            "89.76962339971561\n",
            "length of actions is  1000\n",
            "-19.385666616729203\n",
            "length of actions is  1000\n",
            "-21.527097170160207\n",
            "length of actions is  1000\n",
            "Your final reward is : 35.27\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3040])\n",
            "-3.4962470480214347\n",
            "length of actions is  1000\n",
            "-72.08952372004518\n",
            "length of actions is  192\n",
            "-98.8796338360186\n",
            "length of actions is  238\n",
            "-14.956372848969835\n",
            "length of actions is  1000\n",
            "-80.10380563284639\n",
            "length of actions is  118\n",
            "Your final reward is : -53.91\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2807])\n",
            "-22.965808946708595\n",
            "length of actions is  1000\n",
            "-70.01835989817708\n",
            "length of actions is  241\n",
            "-8.212042582825385\n",
            "length of actions is  1000\n",
            "162.28681079495598\n",
            "length of actions is  778\n",
            "-32.21405492522861\n",
            "length of actions is  1000\n",
            "Your final reward is : 5.78\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2091])\n",
            "-14.536513424992279\n",
            "length of actions is  1000\n",
            "-75.22014230793624\n",
            "length of actions is  217\n",
            "1.040855047101198\n",
            "length of actions is  1000\n",
            "28.691875449297967\n",
            "length of actions is  1000\n",
            "-11.900223322826358\n",
            "length of actions is  1000\n",
            "Your final reward is : -14.38\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2618])\n",
            "-8.313666386671997\n",
            "length of actions is  1000\n",
            "170.44364911696601\n",
            "length of actions is  610\n",
            "-0.06753545677843958\n",
            "length of actions is  1000\n",
            "-95.13374085249126\n",
            "length of actions is  166\n",
            "-10.429220367685637\n",
            "length of actions is  1000\n",
            "Your final reward is : 11.30\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2306])\n",
            "-0.355385070938085\n",
            "length of actions is  1000\n",
            "188.67177716354496\n",
            "length of actions is  526\n",
            "157.62937276390383\n",
            "length of actions is  289\n",
            "37.03360324598498\n",
            "length of actions is  1000\n",
            "2.1371359749010272\n",
            "length of actions is  1000\n",
            "Your final reward is : 77.02\n",
            "torch.from_numpy(rewards) looks like  torch.Size([4448])\n",
            "-9.429859351196223\n",
            "length of actions is  1000\n",
            "155.46486086054955\n",
            "length of actions is  294\n",
            "-7.720137742068935\n",
            "length of actions is  1000\n",
            "1.1107612838105025\n",
            "length of actions is  1000\n",
            "-55.37366692155864\n",
            "length of actions is  190\n",
            "Your final reward is : 16.81\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2705])\n",
            "-5.225720703488652\n",
            "length of actions is  1000\n",
            "168.71044958664098\n",
            "length of actions is  449\n",
            "69.16077373563299\n",
            "length of actions is  1000\n",
            "-98.59657617947394\n",
            "length of actions is  136\n",
            "-8.0211537369963\n",
            "length of actions is  1000\n",
            "Your final reward is : 25.21\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3051])\n",
            "-8.557655991835981\n",
            "length of actions is  1000\n",
            "175.6825679614591\n",
            "length of actions is  633\n",
            "144.84432320643177\n",
            "length of actions is  891\n",
            "3.7926757070311803\n",
            "length of actions is  1000\n",
            "-35.864439352248894\n",
            "length of actions is  1000\n",
            "Your final reward is : 55.98\n",
            "torch.from_numpy(rewards) looks like  torch.Size([4168])\n",
            "5.0365224547783525\n",
            "length of actions is  1000\n",
            "156.1909724263785\n",
            "length of actions is  464\n",
            "-65.08670336813692\n",
            "length of actions is  170\n",
            "-55.47228472726996\n",
            "length of actions is  318\n",
            "19.16265399853769\n",
            "length of actions is  1000\n",
            "Your final reward is : 11.97\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3248])\n",
            "-14.183416357661345\n",
            "length of actions is  1000\n",
            "154.9274095290125\n",
            "length of actions is  409\n",
            "146.71723670100778\n",
            "length of actions is  788\n",
            "179.55748768465844\n",
            "length of actions is  378\n",
            "-98.5547527923639\n",
            "length of actions is  146\n",
            "Your final reward is : 73.69\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2890])\n",
            "-13.856995970684428\n",
            "length of actions is  1000\n",
            "206.96056572520604\n",
            "length of actions is  582\n",
            "182.6870126553688\n",
            "length of actions is  550\n",
            "-10.327024948153777\n",
            "length of actions is  319\n",
            "33.30559118662664\n",
            "length of actions is  1000\n",
            "Your final reward is : 79.75\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3513])\n",
            "-3.5808711247411953\n",
            "length of actions is  1000\n",
            "150.79813969029314\n",
            "length of actions is  420\n",
            "169.9583002929095\n",
            "length of actions is  608\n",
            "224.46407760069133\n",
            "length of actions is  458\n",
            "221.92047909980982\n",
            "length of actions is  461\n",
            "Your final reward is : 152.71\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2721])\n",
            "24.380882863046374\n",
            "length of actions is  1000\n",
            "172.23285630734506\n",
            "length of actions is  395\n",
            "86.51728236442686\n",
            "length of actions is  1000\n",
            "-60.32236417964803\n",
            "length of actions is  372\n",
            "-71.1384703024573\n",
            "length of actions is  149\n",
            "Your final reward is : 30.33\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3351])\n",
            "148.6687358770625\n",
            "length of actions is  720\n",
            "161.33547221570387\n",
            "length of actions is  745\n",
            "209.35227873426948\n",
            "length of actions is  373\n",
            "222.93935867050465\n",
            "length of actions is  347\n",
            "2.2091802968120238\n",
            "length of actions is  1000\n",
            "Your final reward is : 148.90\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1953])\n",
            "-17.03302295927976\n",
            "length of actions is  1000\n",
            "215.54257914639004\n",
            "length of actions is  533\n",
            "216.17115036803415\n",
            "length of actions is  423\n",
            "229.95532873479198\n",
            "length of actions is  390\n",
            "198.83921389338556\n",
            "length of actions is  393\n",
            "Your final reward is : 168.70\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2855])\n",
            "-15.31115590750667\n",
            "length of actions is  1000\n",
            "200.9593801759594\n",
            "length of actions is  611\n",
            "178.28597120897874\n",
            "length of actions is  699\n",
            "221.95321675466684\n",
            "length of actions is  455\n",
            "197.8309234504339\n",
            "length of actions is  367\n",
            "Your final reward is : 156.74\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2595])\n",
            "134.78270868032664\n",
            "length of actions is  773\n",
            "148.5817561308259\n",
            "length of actions is  586\n",
            "174.02591922739487\n",
            "length of actions is  385\n",
            "208.99607339537806\n",
            "length of actions is  384\n",
            "168.96851065773868\n",
            "length of actions is  566\n",
            "Your final reward is : 167.07\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3520])\n",
            "80.82334942496311\n",
            "length of actions is  1000\n",
            "180.2603640897276\n",
            "length of actions is  368\n",
            "202.96532677665152\n",
            "length of actions is  455\n",
            "105.20917044341613\n",
            "length of actions is  1000\n",
            "82.22642160748516\n",
            "length of actions is  1000\n",
            "Your final reward is : 130.30\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2191])\n",
            "-3.620933102966574\n",
            "length of actions is  1000\n",
            "180.73780696172162\n",
            "length of actions is  410\n",
            "211.5015441295673\n",
            "length of actions is  456\n",
            "201.2803389055705\n",
            "length of actions is  540\n",
            "241.978230502173\n",
            "length of actions is  492\n",
            "Your final reward is : 166.38\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2254])\n",
            "87.88673407569345\n",
            "length of actions is  1000\n",
            "-69.84839000234767\n",
            "length of actions is  314\n",
            "161.0126700401017\n",
            "length of actions is  449\n",
            "170.94201024871046\n",
            "length of actions is  837\n",
            "-81.03599565480027\n",
            "length of actions is  139\n",
            "Your final reward is : 53.79\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3245])\n",
            "217.23857068364237\n",
            "length of actions is  525\n",
            "-3.739180848018246\n",
            "length of actions is  1000\n",
            "92.8643001603377\n",
            "length of actions is  1000\n",
            "190.89859514594053\n",
            "length of actions is  427\n",
            "87.67701255293066\n",
            "length of actions is  1000\n",
            "Your final reward is : 116.99\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2110])\n",
            "147.46290413320156\n",
            "length of actions is  908\n",
            "181.29789190460377\n",
            "length of actions is  403\n",
            "210.39375458588927\n",
            "length of actions is  339\n",
            "265.4298875037613\n",
            "length of actions is  270\n",
            "241.96684586045114\n",
            "length of actions is  338\n",
            "Your final reward is : 209.31\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2426])\n",
            "185.9189934267734\n",
            "length of actions is  683\n",
            "-75.07776424061046\n",
            "length of actions is  416\n",
            "-32.21639226844552\n",
            "length of actions is  288\n",
            "174.55880738185067\n",
            "length of actions is  452\n",
            "102.63260157679683\n",
            "length of actions is  1000\n",
            "Your final reward is : 71.16\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2129])\n",
            "25.794824091884408\n",
            "length of actions is  1000\n",
            "151.4977034152477\n",
            "length of actions is  477\n",
            "96.86503211451831\n",
            "length of actions is  750\n",
            "225.56694851921503\n",
            "length of actions is  289\n",
            "232.2388835290934\n",
            "length of actions is  480\n",
            "Your final reward is : 146.39\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2148])\n",
            "80.57478285141501\n",
            "length of actions is  1000\n",
            "158.95980699556486\n",
            "length of actions is  530\n",
            "193.29979819197365\n",
            "length of actions is  396\n",
            "218.75083909122444\n",
            "length of actions is  257\n",
            "200.24389671524239\n",
            "length of actions is  265\n",
            "Your final reward is : 170.37\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1943])\n",
            "-5.619645400183708\n",
            "length of actions is  1000\n",
            "164.99216108636827\n",
            "length of actions is  402\n",
            "215.91542866248858\n",
            "length of actions is  332\n",
            "97.80739887511321\n",
            "length of actions is  1000\n",
            "-98.7008202488693\n",
            "length of actions is  179\n",
            "Your final reward is : 74.88\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2311])\n",
            "-7.109166771624598\n",
            "length of actions is  1000\n",
            "148.38395755852858\n",
            "length of actions is  460\n",
            "282.31007611049176\n",
            "length of actions is  320\n",
            "-37.25060155865171\n",
            "length of actions is  342\n",
            "-70.72001774450705\n",
            "length of actions is  156\n",
            "Your final reward is : 63.12\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2847])\n",
            "200.70015389736295\n",
            "length of actions is  619\n",
            "-99.66546479441558\n",
            "length of actions is  160\n",
            "268.13137355649917\n",
            "length of actions is  291\n",
            "266.91810819001535\n",
            "length of actions is  289\n",
            "212.10399079332157\n",
            "length of actions is  333\n",
            "Your final reward is : 169.64\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1607])\n",
            "66.29117161651234\n",
            "length of actions is  1000\n",
            "152.23757150297274\n",
            "length of actions is  447\n",
            "201.05939639193764\n",
            "length of actions is  397\n",
            "255.39727231810267\n",
            "length of actions is  371\n",
            "-89.0999164936281\n",
            "length of actions is  172\n",
            "Your final reward is : 117.18\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2114])\n",
            "221.80999133501865\n",
            "length of actions is  297\n",
            "247.5863289084943\n",
            "length of actions is  962\n",
            "-36.256796255676115\n",
            "length of actions is  328\n",
            "84.60655156070784\n",
            "length of actions is  1000\n",
            "-85.77877479469394\n",
            "length of actions is  153\n",
            "Your final reward is : 86.39\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2719])\n",
            "209.1613273228432\n",
            "length of actions is  577\n",
            "196.57968648231034\n",
            "length of actions is  375\n",
            "270.6682197525861\n",
            "length of actions is  284\n",
            "-35.09182430363849\n",
            "length of actions is  292\n",
            "260.55297501285224\n",
            "length of actions is  341\n",
            "Your final reward is : 180.37\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1534])\n",
            "29.422870371675884\n",
            "length of actions is  244\n",
            "-37.22980726539986\n",
            "length of actions is  469\n",
            "-43.73350901720709\n",
            "length of actions is  609\n",
            "212.76480049159954\n",
            "length of actions is  320\n",
            "252.70966563121277\n",
            "length of actions is  357\n",
            "Your final reward is : 82.79\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1814])\n",
            "29.61309179686546\n",
            "length of actions is  243\n",
            "218.87179830231025\n",
            "length of actions is  409\n",
            "-40.206904434964194\n",
            "length of actions is  323\n",
            "194.14080776338943\n",
            "length of actions is  568\n",
            "252.46952477546046\n",
            "length of actions is  461\n",
            "Your final reward is : 130.98\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2114])\n",
            "246.41947248233174\n",
            "length of actions is  319\n",
            "-58.467958454167714\n",
            "length of actions is  315\n",
            "167.74166443973832\n",
            "length of actions is  438\n",
            "-105.82420444203346\n",
            "length of actions is  164\n",
            "192.6969905719188\n",
            "length of actions is  764\n",
            "Your final reward is : 88.51\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2320])\n",
            "239.32173371829242\n",
            "length of actions is  306\n",
            "192.1702950746057\n",
            "length of actions is  440\n",
            "267.78956508918463\n",
            "length of actions is  280\n",
            "181.57425399762985\n",
            "length of actions is  443\n",
            "201.05013823947195\n",
            "length of actions is  374\n",
            "Your final reward is : 216.38\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2089])\n",
            "202.76901824590848\n",
            "length of actions is  534\n",
            "-59.904179696266596\n",
            "length of actions is  396\n",
            "232.15230616638195\n",
            "length of actions is  406\n",
            "179.8580787601398\n",
            "length of actions is  312\n",
            "-73.52642706738138\n",
            "length of actions is  324\n",
            "Your final reward is : 96.27\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2390])\n",
            "115.0738621697119\n",
            "length of actions is  1000\n",
            "162.64116951683403\n",
            "length of actions is  436\n",
            "221.74586477520586\n",
            "length of actions is  313\n",
            "-38.49163053612486\n",
            "length of actions is  247\n",
            "219.79110665382808\n",
            "length of actions is  615\n",
            "Your final reward is : 136.15\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2239])\n",
            "226.3593018781179\n",
            "length of actions is  526\n",
            "-29.074358909622802\n",
            "length of actions is  284\n",
            "172.93858984804615\n",
            "length of actions is  444\n",
            "14.783627651701039\n",
            "length of actions is  267\n",
            "-44.69451320410808\n",
            "length of actions is  303\n",
            "Your final reward is : 68.06\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2570])\n",
            "233.54676508863056\n",
            "length of actions is  512\n",
            "-22.311169962553763\n",
            "length of actions is  404\n",
            "-79.48289385773379\n",
            "length of actions is  362\n",
            "207.6070534446841\n",
            "length of actions is  328\n",
            "79.19814037667935\n",
            "length of actions is  1000\n",
            "Your final reward is : 83.71\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2471])\n",
            "230.53664215154996\n",
            "length of actions is  529\n",
            "-5.832509035608481\n",
            "length of actions is  373\n",
            "19.255701565479413\n",
            "length of actions is  428\n",
            "249.93682890807895\n",
            "length of actions is  261\n",
            "234.19147136502443\n",
            "length of actions is  339\n",
            "Your final reward is : 145.62\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1871])\n",
            "120.53834382018219\n",
            "length of actions is  1000\n",
            "165.94631574623162\n",
            "length of actions is  347\n",
            "262.663975890949\n",
            "length of actions is  417\n",
            "96.49925795413233\n",
            "length of actions is  1000\n",
            "210.85291560079799\n",
            "length of actions is  329\n",
            "Your final reward is : 171.30\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2345])\n",
            "112.92776820695396\n",
            "length of actions is  1000\n",
            "-66.44091365200214\n",
            "length of actions is  275\n",
            "266.938026378623\n",
            "length of actions is  386\n",
            "265.15030535153346\n",
            "length of actions is  364\n",
            "191.7963188060961\n",
            "length of actions is  424\n",
            "Your final reward is : 154.07\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2285])\n",
            "225.91369572447715\n",
            "length of actions is  494\n",
            "225.64397052440015\n",
            "length of actions is  315\n",
            "86.21088601493094\n",
            "length of actions is  1000\n",
            "123.7039392822849\n",
            "length of actions is  1000\n",
            "253.83444807124167\n",
            "length of actions is  349\n",
            "Your final reward is : 183.06\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2196])\n",
            "255.63859436816423\n",
            "length of actions is  324\n",
            "187.73079610747624\n",
            "length of actions is  385\n",
            "268.27201627742494\n",
            "length of actions is  433\n",
            "135.50955160126802\n",
            "length of actions is  390\n",
            "245.0197018893012\n",
            "length of actions is  379\n",
            "Your final reward is : 218.43\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2905])\n",
            "222.83806736607775\n",
            "length of actions is  544\n",
            "139.2181504487628\n",
            "length of actions is  1000\n",
            "121.90222410566852\n",
            "length of actions is  1000\n",
            "116.09386321605204\n",
            "length of actions is  1000\n",
            "7.999422213495706\n",
            "length of actions is  232\n",
            "Your final reward is : 121.61\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1902])\n",
            "226.40496962350142\n",
            "length of actions is  514\n",
            "-18.322074423947114\n",
            "length of actions is  229\n",
            "218.289620835936\n",
            "length of actions is  532\n",
            "221.74275083695673\n",
            "length of actions is  307\n",
            "201.4546901299388\n",
            "length of actions is  424\n",
            "Your final reward is : 169.91\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2040])\n",
            "7.012668611822619\n",
            "length of actions is  297\n",
            "200.70772457873693\n",
            "length of actions is  715\n",
            "219.37568702316548\n",
            "length of actions is  417\n",
            "237.37580373336814\n",
            "length of actions is  567\n",
            "219.43853788196395\n",
            "length of actions is  317\n",
            "Your final reward is : 176.78\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2576])\n",
            "225.08621172120974\n",
            "length of actions is  528\n",
            "150.3395648465065\n",
            "length of actions is  1000\n",
            "180.37774953878028\n",
            "length of actions is  518\n",
            "226.93433270491266\n",
            "length of actions is  290\n",
            "280.8443274303014\n",
            "length of actions is  265\n",
            "Your final reward is : 212.72\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2405])\n",
            "229.36243652912384\n",
            "length of actions is  500\n",
            "200.89925389394304\n",
            "length of actions is  445\n",
            "192.31072247979913\n",
            "length of actions is  507\n",
            "238.24962348674111\n",
            "length of actions is  570\n",
            "-5.604441841147221\n",
            "length of actions is  304\n",
            "Your final reward is : 171.04\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2556])\n",
            "192.84792299078597\n",
            "length of actions is  690\n",
            "161.8489150526305\n",
            "length of actions is  596\n",
            "212.97642819236148\n",
            "length of actions is  340\n",
            "214.77738129213913\n",
            "length of actions is  367\n",
            "-11.733161260149643\n",
            "length of actions is  255\n",
            "Your final reward is : 154.14\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2254])\n",
            "217.40011030939627\n",
            "length of actions is  527\n",
            "241.18105531673834\n",
            "length of actions is  498\n",
            "208.63063059244394\n",
            "length of actions is  324\n",
            "219.7185944273844\n",
            "length of actions is  719\n",
            "244.03815639310454\n",
            "length of actions is  234\n",
            "Your final reward is : 226.19\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2244])\n",
            "226.08373877434752\n",
            "length of actions is  540\n",
            "218.17937669677715\n",
            "length of actions is  302\n",
            "-5.248612977999656\n",
            "length of actions is  377\n",
            "232.9888919190119\n",
            "length of actions is  347\n",
            "243.89786020250065\n",
            "length of actions is  433\n",
            "Your final reward is : 183.18\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1705])\n",
            "226.159882198504\n",
            "length of actions is  554\n",
            "262.5540685391611\n",
            "length of actions is  470\n",
            "136.41386010032465\n",
            "length of actions is  1000\n",
            "251.46855181053226\n",
            "length of actions is  530\n",
            "23.371949551814566\n",
            "length of actions is  171\n",
            "Your final reward is : 179.99\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2256])\n",
            "229.31905132029127\n",
            "length of actions is  350\n",
            "35.25171116813206\n",
            "length of actions is  200\n",
            "222.91174972142846\n",
            "length of actions is  387\n",
            "186.89015119299174\n",
            "length of actions is  516\n",
            "238.40373164668188\n",
            "length of actions is  304\n",
            "Your final reward is : 182.56\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3251])\n",
            "14.890506843565703\n",
            "length of actions is  200\n",
            "-53.855929763430225\n",
            "length of actions is  151\n",
            "115.27360330035323\n",
            "length of actions is  666\n",
            "213.24590919447581\n",
            "length of actions is  252\n",
            "-127.64946112860875\n",
            "length of actions is  160\n",
            "Your final reward is : 32.38\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2779])\n",
            "218.67578014229173\n",
            "length of actions is  550\n",
            "13.72772506757424\n",
            "length of actions is  184\n",
            "189.0518826704443\n",
            "length of actions is  330\n",
            "229.35768126059682\n",
            "length of actions is  479\n",
            "163.87262079028682\n",
            "length of actions is  1000\n",
            "Your final reward is : 162.94\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2456])\n",
            "227.29258848820115\n",
            "length of actions is  516\n",
            "35.46325481272089\n",
            "length of actions is  204\n",
            "248.98330197758494\n",
            "length of actions is  239\n",
            "45.32244431777721\n",
            "length of actions is  186\n",
            "292.1992278093982\n",
            "length of actions is  229\n",
            "Your final reward is : 169.85\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1728])\n",
            "225.9289762304297\n",
            "length of actions is  511\n",
            "146.1343683461697\n",
            "length of actions is  525\n",
            "185.18324857127047\n",
            "length of actions is  534\n",
            "45.42664245425263\n",
            "length of actions is  194\n",
            "208.82195779343772\n",
            "length of actions is  278\n",
            "Your final reward is : 162.30\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2173])\n",
            "228.63530026025413\n",
            "length of actions is  261\n",
            "180.8823469865224\n",
            "length of actions is  298\n",
            "33.89191795558122\n",
            "length of actions is  196\n",
            "6.727752681235955\n",
            "length of actions is  218\n",
            "105.26375558904064\n",
            "length of actions is  1000\n",
            "Your final reward is : 111.08\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2180])\n",
            "9.041671365218221\n",
            "length of actions is  212\n",
            "123.12760558537744\n",
            "length of actions is  655\n",
            "4.977761047508082\n",
            "length of actions is  167\n",
            "207.63216607108558\n",
            "length of actions is  721\n",
            "-19.130923836141847\n",
            "length of actions is  133\n",
            "Your final reward is : 65.13\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1076])\n",
            "28.279947853690032\n",
            "length of actions is  205\n",
            "13.756842988667472\n",
            "length of actions is  172\n",
            "136.30327838406083\n",
            "length of actions is  536\n",
            "159.2117805307757\n",
            "length of actions is  427\n",
            "188.08544684690884\n",
            "length of actions is  623\n",
            "Your final reward is : 105.13\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1952])\n",
            "27.546346801132444\n",
            "length of actions is  207\n",
            "-40.201297422402064\n",
            "length of actions is  170\n",
            "134.64575909167257\n",
            "length of actions is  504\n",
            "48.29829674951499\n",
            "length of actions is  1000\n",
            "9.810094866268557\n",
            "length of actions is  201\n",
            "Your final reward is : 36.02\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1477])\n",
            "24.434336253492177\n",
            "length of actions is  203\n",
            "178.71276449951108\n",
            "length of actions is  419\n",
            "-3.7755841778112966\n",
            "length of actions is  154\n",
            "29.52082108268405\n",
            "length of actions is  201\n",
            "-18.894458712187827\n",
            "length of actions is  226\n",
            "Your final reward is : 42.00\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1959])\n",
            "27.508529814895383\n",
            "length of actions is  211\n",
            "-12.655166880607723\n",
            "length of actions is  156\n",
            "53.24782237929858\n",
            "length of actions is  185\n",
            "224.05851328450464\n",
            "length of actions is  329\n",
            "54.17457636706044\n",
            "length of actions is  182\n",
            "Your final reward is : 69.27\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1554])\n",
            "46.628106126211236\n",
            "length of actions is  205\n",
            "22.261499584463095\n",
            "length of actions is  176\n",
            "223.13625857964718\n",
            "length of actions is  400\n",
            "236.47627425387046\n",
            "length of actions is  667\n",
            "-27.604846733011186\n",
            "length of actions is  111\n",
            "Your final reward is : 100.18\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1720])\n",
            "269.5713993438681\n",
            "length of actions is  254\n",
            "142.4032062380742\n",
            "length of actions is  1000\n",
            "141.59827498975375\n",
            "length of actions is  550\n",
            "273.52557292654717\n",
            "length of actions is  312\n",
            "256.945626093068\n",
            "length of actions is  220\n",
            "Your final reward is : 216.81\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2003])\n",
            "-9.42761283383555\n",
            "length of actions is  1000\n",
            "183.9890867452301\n",
            "length of actions is  352\n",
            "12.215145732957197\n",
            "length of actions is  190\n",
            "67.95969461994896\n",
            "length of actions is  1000\n",
            "12.00975802368491\n",
            "length of actions is  174\n",
            "Your final reward is : 53.35\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1446])\n",
            "257.52493466153743\n",
            "length of actions is  326\n",
            "27.296742849089323\n",
            "length of actions is  1000\n",
            "-138.11446876051778\n",
            "length of actions is  350\n",
            "201.57460743322346\n",
            "length of actions is  688\n",
            "195.86811461660614\n",
            "length of actions is  265\n",
            "Your final reward is : 108.83\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2435])\n",
            "228.42156238042537\n",
            "length of actions is  486\n",
            "199.37540885581979\n",
            "length of actions is  413\n",
            "189.80070933953448\n",
            "length of actions is  333\n",
            "174.80969139803636\n",
            "length of actions is  756\n",
            "169.6781714425918\n",
            "length of actions is  691\n",
            "Your final reward is : 192.42\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3284])\n",
            "215.41614241479684\n",
            "length of actions is  564\n",
            "228.3663705519865\n",
            "length of actions is  339\n",
            "215.83346698411464\n",
            "length of actions is  290\n",
            "30.857229578607438\n",
            "length of actions is  1000\n",
            "218.4757825849456\n",
            "length of actions is  297\n",
            "Your final reward is : 181.79\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2084])\n",
            "227.26064168752492\n",
            "length of actions is  510\n",
            "218.3331928026738\n",
            "length of actions is  477\n",
            "220.3352198718683\n",
            "length of actions is  354\n",
            "215.98182825597533\n",
            "length of actions is  296\n",
            "210.4128141044675\n",
            "length of actions is  315\n",
            "Your final reward is : 218.46\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1908])\n",
            "223.56729013834763\n",
            "length of actions is  512\n",
            "106.58092536198355\n",
            "length of actions is  1000\n",
            "245.03692988925243\n",
            "length of actions is  311\n",
            "245.12442909720312\n",
            "length of actions is  333\n",
            "234.73764372226037\n",
            "length of actions is  326\n",
            "Your final reward is : 211.01\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1686])\n",
            "241.01092725129482\n",
            "length of actions is  414\n",
            "239.31285974425313\n",
            "length of actions is  377\n",
            "219.72868304910486\n",
            "length of actions is  331\n",
            "21.33612988116971\n",
            "length of actions is  354\n",
            "226.9850393550846\n",
            "length of actions is  362\n",
            "Your final reward is : 189.67\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2243])\n",
            "-3.0227125693615857\n",
            "length of actions is  1000\n",
            "218.86908135131972\n",
            "length of actions is  321\n",
            "214.16317024296194\n",
            "length of actions is  675\n",
            "-30.36965779830101\n",
            "length of actions is  1000\n",
            "199.06086107744437\n",
            "length of actions is  325\n",
            "Your final reward is : 119.74\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2398])\n",
            "3.484564986854622\n",
            "length of actions is  1000\n",
            "218.39740669126036\n",
            "length of actions is  440\n",
            "-2.19822190063349\n",
            "length of actions is  1000\n",
            "147.23515002425748\n",
            "length of actions is  1000\n",
            "253.9149317975592\n",
            "length of actions is  426\n",
            "Your final reward is : 124.17\n",
            "torch.from_numpy(rewards) looks like  torch.Size([2619])\n",
            "2.400482306773978\n",
            "length of actions is  1000\n",
            "217.3272508648834\n",
            "length of actions is  550\n",
            "195.47703454704936\n",
            "length of actions is  339\n",
            "-10.556883994213635\n",
            "length of actions is  1000\n",
            "215.97715292114864\n",
            "length of actions is  371\n",
            "Your final reward is : 124.13\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3321])\n",
            "-22.919240647176917\n",
            "length of actions is  1000\n",
            "224.76699555936327\n",
            "length of actions is  329\n",
            "235.50079415084798\n",
            "length of actions is  435\n",
            "248.89003599091612\n",
            "length of actions is  923\n",
            "110.5621807347543\n",
            "length of actions is  1000\n",
            "Your final reward is : 159.36\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3436])\n",
            "-5.149441428843795\n",
            "length of actions is  1000\n",
            "-52.024893306355565\n",
            "length of actions is  212\n",
            "136.38966139105406\n",
            "length of actions is  1000\n",
            "6.443685899649451\n",
            "length of actions is  1000\n",
            "231.00490272683314\n",
            "length of actions is  349\n",
            "Your final reward is : 63.33\n",
            "torch.from_numpy(rewards) looks like  torch.Size([3415])\n",
            "134.57644159546308\n",
            "length of actions is  1000\n",
            "228.59662746014365\n",
            "length of actions is  557\n",
            "187.42140460466425\n",
            "length of actions is  958\n",
            "192.69790446984348\n",
            "length of actions is  461\n",
            "30.81998083739782\n",
            "length of actions is  203\n",
            "Your final reward is : 154.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNb_tuFYhKVK"
      },
      "source": [
        "### Training Result\n",
        "During the training process, we recorded `avg_total_reward`, which represents the average total reward of episodes before updating the policy network.\n",
        "\n",
        "Theoretically, if the agent becomes better, the `avg_total_reward` will increase.\n",
        "The visualization of the training process is shown below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZYOI8H10SHN"
      },
      "source": [
        "plt.plot(avg_total_rewards)\n",
        "plt.title(\"Total Rewards\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV5jj4dThz0Y"
      },
      "source": [
        "In addition, `avg_final_reward` represents average final rewards of episodes. To be specific, final rewards is the last reward received in one episode, indicating whether the craft lands successfully or not.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txDZ5vlGWz5w"
      },
      "source": [
        "plt.plot(avg_final_rewards)\n",
        "plt.title(\"Final Rewards\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2HaGRVEYGQS"
      },
      "source": [
        "## Testing\n",
        "The testing result will be the average reward of 5 testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yFuUKKRYH73"
      },
      "source": [
        "fix(env, seed)\n",
        "agent.q_target.eval()\n",
        "#agent.network.eval()  # set the network into evaluation mode\n",
        "NUM_OF_TEST = 5 # Do not revise this !!!\n",
        "test_total_reward = []\n",
        "action_list = []\n",
        "for i in range(NUM_OF_TEST):\n",
        "  actions = []\n",
        "  state = env.reset()\n",
        "\n",
        "  img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "      action, _ = agent.sample(state)\n",
        "      actions.append(action)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "\n",
        "      total_reward += reward\n",
        "\n",
        "      img.set_data(env.render(mode='rgb_array'))\n",
        "      display.display(plt.gcf())\n",
        "      display.clear_output(wait=True)\n",
        "\n",
        "  print(total_reward)\n",
        "  test_total_reward.append(total_reward)\n",
        "\n",
        "  action_list.append(actions) # save the result of testing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aex7mcKr0J01"
      },
      "source": [
        "print(np.mean(test_total_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leyebGYRpqsF"
      },
      "source": [
        "Action list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGAH4YWDpp4u"
      },
      "source": [
        "print(\"Action list looks like \", action_list)\n",
        "print(\"Action list's shape looks like \", np.shape(action_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNkmwucrHMen"
      },
      "source": [
        "Analysis of actions taken by agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHdAItjj1nxw"
      },
      "source": [
        "distribution = {}\n",
        "for actions in action_list:\n",
        "  for action in actions:\n",
        "    if action not in distribution.keys():\n",
        "      distribution[action] = 1\n",
        "    else:\n",
        "      distribution[action] += 1\n",
        "print(distribution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ricE0schY75M"
      },
      "source": [
        "Saving the result of Model Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZsMkGmIY42b"
      },
      "source": [
        "PATH = \"Action_List.npy\" # Can be modified into the name or path you want\n",
        "np.save(PATH ,np.array(action_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asK7WfbkaLjt"
      },
      "source": [
        "### This is the file you need to submit !!!\n",
        "Download the testing result to your device\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-CqyhHzaWAL"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seT4NUmWmAZ1"
      },
      "source": [
        "# Server\n",
        "The code below simulate the environment on the judge server. Can be used for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U69c-YTxaw6b"
      },
      "source": [
        "action_list = np.load(PATH,allow_pickle=True) # The action list you upload\n",
        "seed = 2023 # Do not revise this\n",
        "fix(env, seed)\n",
        "\n",
        "agent.network.eval()  # set network to evaluation mode\n",
        "\n",
        "test_total_reward = []\n",
        "if len(action_list) != 5:\n",
        "  print(\"Wrong format of file !!!\")\n",
        "  exit(0)\n",
        "for actions in action_list:\n",
        "  state = env.reset()\n",
        "  img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "\n",
        "  for action in actions:\n",
        "\n",
        "      state, reward, done, _ = env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "  print(f\"Your reward is : %.2f\"%total_reward)\n",
        "  test_total_reward.append(total_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjFBWwQP1hVe"
      },
      "source": [
        "# Your score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpJpZz3Wbm0X"
      },
      "source": [
        "print(f\"Your final reward is : %.2f\"%np.mean(test_total_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUBtYXG2eaqf"
      },
      "source": [
        "## Reference\n",
        "\n",
        "Below are some useful tips for you to get high score.\n",
        "\n",
        "- [DRL Lecture 1: Policy Gradient (Review)](https://youtu.be/z95ZYgPgXOY)\n",
        "- [ML Lecture 23-3: Reinforcement Learning (including Q-learning) start at 30:00](https://youtu.be/2-JNBzCq77c?t=1800)\n",
        "- [Lecture 7: Policy Gradient, David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)\n"
      ]
    }
  ]
}