{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partaa\n!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partab\n!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partac\n!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partad\n\n!cat Dataset.tar.gz.part* > Dataset.tar.gz\n!rm Dataset.tar.gz.partaa\n!rm Dataset.tar.gz.partab\n!rm Dataset.tar.gz.partac\n!rm Dataset.tar.gz.partad\n# unzip the file\n!tar zxf Dataset.tar.gz\n!rm Dataset.tar.gz","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-07T08:52:58.019017Z","iopub.execute_input":"2023-04-07T08:52:58.019990Z","iopub.status.idle":"2023-04-07T08:58:04.171186Z","shell.execute_reply.started":"2023-04-07T08:52:58.019925Z","shell.execute_reply":"2023-04-07T08:58:04.169800Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"--2023-04-07 08:52:58--  https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partaa\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/7646b36b-6033-4a31-bac4-380c4d21d91e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230407T085056Z&X-Amz-Expires=300&X-Amz-Signature=8a78bc5e285302b9b360cf574b9eee3843ac25adcb79da473932711aacf57633&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partaa&response-content-type=application%2Foctet-stream [following]\n--2023-04-07 08:52:59--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/7646b36b-6033-4a31-bac4-380c4d21d91e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230407T085056Z&X-Amz-Expires=300&X-Amz-Signature=8a78bc5e285302b9b360cf574b9eee3843ac25adcb79da473932711aacf57633&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partaa&response-content-type=application%2Foctet-stream\nResolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1560784333 (1.5G) [application/octet-stream]\nSaving to: ‘Dataset.tar.gz.partaa’\n\nDataset.tar.gz.part 100%[===================>]   1.45G  37.3MB/s    in 41s     \n\n2023-04-07 08:53:40 (36.6 MB/s) - ‘Dataset.tar.gz.partaa’ saved [1560784333/1560784333]\n\n--2023-04-07 08:53:41--  https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partab\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/95b45712-6e2f-4a52-96b1-7d88578345fc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230407T085341Z&X-Amz-Expires=300&X-Amz-Signature=de429b3b3721d3a15696f981becc23ba0d8466cc37b005b496d04dfcfbce9a39&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partab&response-content-type=application%2Foctet-stream [following]\n--2023-04-07 08:53:41--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/95b45712-6e2f-4a52-96b1-7d88578345fc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230407T085341Z&X-Amz-Expires=300&X-Amz-Signature=de429b3b3721d3a15696f981becc23ba0d8466cc37b005b496d04dfcfbce9a39&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partab&response-content-type=application%2Foctet-stream\nResolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1560784333 (1.5G) [application/octet-stream]\nSaving to: ‘Dataset.tar.gz.partab’\n\nDataset.tar.gz.part 100%[===================>]   1.45G  36.4MB/s    in 41s     \n\n2023-04-07 08:54:23 (36.1 MB/s) - ‘Dataset.tar.gz.partab’ saved [1560784333/1560784333]\n\n--2023-04-07 08:54:24--  https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partac\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/0c9d42d3-95b7-4ca4-b57c-ab1a66a5564d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230407T085425Z&X-Amz-Expires=300&X-Amz-Signature=3fc005e9021f43a0c8bee30a98399063d1d7427c64f3b0ea6707726d678ffc8c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partac&response-content-type=application%2Foctet-stream [following]\n--2023-04-07 08:54:25--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/0c9d42d3-95b7-4ca4-b57c-ab1a66a5564d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230407T085425Z&X-Amz-Expires=300&X-Amz-Signature=3fc005e9021f43a0c8bee30a98399063d1d7427c64f3b0ea6707726d678ffc8c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partac&response-content-type=application%2Foctet-stream\nResolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1560784333 (1.5G) [application/octet-stream]\nSaving to: ‘Dataset.tar.gz.partac’\n\nDataset.tar.gz.part 100%[===================>]   1.45G  38.1MB/s    in 40s     \n\n2023-04-07 08:55:06 (36.8 MB/s) - ‘Dataset.tar.gz.partac’ saved [1560784333/1560784333]\n\n--2023-04-07 08:55:07--  https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partad\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/0ee11da6-8c96-4463-b084-cea8f95d26e9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230407T085507Z&X-Amz-Expires=300&X-Amz-Signature=c2fd41ed94355373b883c2383fb67eb648b6c5c2a2f70bec5616e93b417daea8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partad&response-content-type=application%2Foctet-stream [following]\n--2023-04-07 08:55:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/0ee11da6-8c96-4463-b084-cea8f95d26e9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230407T085507Z&X-Amz-Expires=300&X-Amz-Signature=c2fd41ed94355373b883c2383fb67eb648b6c5c2a2f70bec5616e93b417daea8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partad&response-content-type=application%2Foctet-stream\nResolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1560784336 (1.5G) [application/octet-stream]\nSaving to: ‘Dataset.tar.gz.partad’\n\nDataset.tar.gz.part 100%[===================>]   1.45G  37.7MB/s    in 39s     \n\n2023-04-07 08:55:47 (37.8 MB/s) - ‘Dataset.tar.gz.partad’ saved [1560784336/1560784336]\n\ntar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.macl'\n","output_type":"stream"}]},{"cell_type":"code","source":"!tar zxf Dataset.tar.gz","metadata":{"execution":{"iopub.status.busy":"2023-04-07T08:58:04.175101Z","iopub.execute_input":"2023-04-07T08:58:04.175427Z","iopub.status.idle":"2023-04-07T08:58:05.289871Z","shell.execute_reply.started":"2023-04-07T08:58:04.175395Z","shell.execute_reply":"2023-04-07T08:58:05.288684Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tar (child): Dataset.tar.gz: Cannot open: No such file or directory\ntar (child): Error is not recoverable: exiting now\ntar: Child returned status 2\ntar: Error is not recoverable: exiting now\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport random\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nset_seed(87)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T08:58:05.291741Z","iopub.execute_input":"2023-04-07T08:58:05.293266Z","iopub.status.idle":"2023-04-07T08:58:05.304963Z","shell.execute_reply.started":"2023-04-07T08:58:05.293222Z","shell.execute_reply":"2023-04-07T08:58:05.303876Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport random\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n \n \nclass myDataset(Dataset):\n    def __init__(self, data_dir, segment_len=128):\n        self.data_dir = data_dir\n        self.segment_len = segment_len\n    \n        # Load the mapping from speaker neme to their corresponding id. \n        mapping_path = Path(data_dir) / \"mapping.json\"\n        mapping = json.load(mapping_path.open())\n        self.speaker2id = mapping[\"speaker2id\"]\n    \n        # Load metadata of training data.\n        metadata_path = Path(data_dir) / \"metadata.json\"\n        metadata = json.load(open(metadata_path))[\"speakers\"]\n\n        # Get the total number of speaker.\n        self.speaker_num = len(metadata.keys())\n        self.data = []\n        for speaker in metadata.keys():\n            for utterances in metadata[speaker]:\n                self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n \n    def __len__(self):\n        return len(self.data)\n \n    def __getitem__(self, index):\n        feat_path, speaker = self.data[index]\n        # Load preprocessed mel-spectrogram.\n        mel = torch.load(os.path.join(self.data_dir, feat_path))\n\n        # Segmemt mel-spectrogram into \"segment_len\" frames.\n        if len(mel) > self.segment_len:\n            # Randomly get the starting point of the segment.\n            start = random.randint(0, len(mel) - self.segment_len)\n            # Get a segment with \"segment_len\" frames.\n            mel = torch.FloatTensor(mel[start:start+self.segment_len])\n        else:\n            mel = torch.FloatTensor(mel)\n        # Turn the speaker id into long for computing loss later.\n        speaker = torch.FloatTensor([speaker]).long()\n        return mel, speaker\n \n    def get_speaker_number(self):\n        return self.speaker_num","metadata":{"execution":{"iopub.status.busy":"2023-04-07T08:58:05.308059Z","iopub.execute_input":"2023-04-07T08:58:05.308660Z","iopub.status.idle":"2023-04-07T08:58:05.320573Z","shell.execute_reply.started":"2023-04-07T08:58:05.308620Z","shell.execute_reply":"2023-04-07T08:58:05.319420Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef collate_batch(batch):\n\t# Process features within a batch.\n\t\"\"\"Collate a batch of data.\"\"\"\n\tmel, speaker = zip(*batch)\n\t# Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n\tmel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n\t# mel: (batch size, length, 40)\n\treturn mel, torch.FloatTensor(speaker).long()\n\n\ndef get_dataloader(data_dir, batch_size, n_workers):\n\t\"\"\"Generate dataloader\"\"\"\n\tdataset = myDataset(data_dir)\n\tspeaker_num = dataset.get_speaker_number()\n\t# Split dataset into training dataset and validation dataset\n\ttrainlen = int(0.9 * len(dataset))\n\tlengths = [trainlen, len(dataset) - trainlen]\n\ttrainset, validset = random_split(dataset, lengths)\n\n\ttrain_loader = DataLoader(\n\t\ttrainset,\n\t\tbatch_size=batch_size,\n\t\tshuffle=True,\n\t\tdrop_last=True,\n\t\tnum_workers=n_workers,\n\t\tpin_memory=True,\n\t\tcollate_fn=collate_batch,\n\t)\n\tvalid_loader = DataLoader(\n\t\tvalidset,\n\t\tbatch_size=batch_size,\n\t\tnum_workers=n_workers,\n\t\tdrop_last=True,\n\t\tpin_memory=True,\n\t\tcollate_fn=collate_batch,\n\t)\n\n\treturn train_loader, valid_loader, speaker_num","metadata":{"execution":{"iopub.status.busy":"2023-04-07T08:58:05.322328Z","iopub.execute_input":"2023-04-07T08:58:05.322713Z","iopub.status.idle":"2023-04-07T08:58:05.335151Z","shell.execute_reply.started":"2023-04-07T08:58:05.322678Z","shell.execute_reply":"2023-04-07T08:58:05.334006Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n!pip install conformer\nfrom conformer import ConformerBlock\n\"\"\"\nRef:\nhttps://zhuanlan.zhihu.com/p/439212197\nhttps://github.com/lucidrains/conformer/blob/master/conformer/conformer.py\nhttps://github.com/sooftware/conformer\n\"\"\"\nclass Classifier(nn.Module):\n\tdef __init__(self, d_model=160, n_spks=600, dropout=0.2):\n\t\tsuper().__init__()\n\t\t# Project the dimension of features from that of input into d_model.\n\t\tself.prenet = nn.Linear(40, d_model)\n\t\t# TODO:\n\t\t#   Change Transformer to Conformer.\n\t\t#   https://arxiv.org/abs/2005.08100\n\t\t\n\t\t#self.conv_layer = nn.Conv1d(d_model,32,kernel_size=3, stride=1, padding=1)\n        #self.encoder_layer = nn.TransformerEncoderLayer(\n        #    d_model=d_model, dim_feedforward=256, nhead=2\n        #)\n        \n\t\tself.conformer = ConformerBlock(\n            dim = d_model,\n            dim_head = 64,\n            heads = 1,\n            ff_mult = 4,\n            conv_expansion_factor = 4,\n            conv_kernel_size = 31,\n            attn_dropout = dropout,\n            ff_dropout = dropout,\n            conv_dropout = dropout\n        )\n        \n\t\t#self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n\n\t\t# Project the the dimension of features from d_model into speaker nums.\n\t\tself.pred_layer = nn.Sequential(\n\t\t\t#nn.Linear(d_model, d_model),\n\t\t\t#nn.Sigmoid(),\n\t\t\tnn.Linear(d_model, n_spks),\n\t\t)\n\n\tdef forward(self, mels):\n\t\t\"\"\"\n\t\targs:\n\t\t\tmels: (batch size, length, 40)\n\t\treturn:\n\t\t\tout: (batch size, n_spks)\n\t\t\"\"\"\n\t\t# out: (batch size, length, d_model)\n\t\tout = self.prenet(mels)\n\t\t# out: (length, batch size, d_model)\n\t\tout = out.permute(1, 0, 2)\n\t\t# The encoder layer expect features in the shape of (length, batch size, d_model).\n\t\tout = self.conformer(out)\n        #out = self.encoder(out)\n\t\t# out: (batch size, length, d_model)\n\t\tout = out.transpose(0, 1)\n\t\t# mean pooling\n\t\tstats = out.mean(dim=1)\n\n\t\t# out: (batch, n_spks)\n\t\tout = self.pred_layer(stats)\n\t\treturn out\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:22:09.649133Z","iopub.execute_input":"2023-04-07T09:22:09.649531Z","iopub.status.idle":"2023-04-07T09:22:19.378412Z","shell.execute_reply.started":"2023-04-07T09:22:09.649494Z","shell.execute_reply":"2023-04-07T09:22:19.377107Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Requirement already satisfied: conformer in /opt/conda/lib/python3.7/site-packages (0.2.5)\nRequirement already satisfied: einops in /opt/conda/lib/python3.7/site-packages (from conformer) (0.6.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from conformer) (1.13.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->conformer) (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import math\n\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\ndef get_cosine_schedule_with_warmup(\n\toptimizer: Optimizer,\n\tnum_warmup_steps: int,\n\tnum_training_steps: int,\n\tnum_cycles: float = 0.5,\n\tlast_epoch: int = -1,\n):\n\t\"\"\"\n\tCreate a schedule with a learning rate that decreases following the values of the cosine function between the\n\tinitial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n\tinitial lr set in the optimizer.\n\n\tArgs:\n\t\toptimizer (:class:`~torch.optim.Optimizer`):\n\t\tThe optimizer for which to schedule the learning rate.\n\t\tnum_warmup_steps (:obj:`int`):\n\t\tThe number of steps for the warmup phase.\n\t\tnum_training_steps (:obj:`int`):\n\t\tThe total number of training steps.\n\t\tnum_cycles (:obj:`float`, `optional`, defaults to 0.5):\n\t\tThe number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n\t\tfollowing a half-cosine).\n\t\tlast_epoch (:obj:`int`, `optional`, defaults to -1):\n\t\tThe index of the last epoch when resuming training.\n\n\tReturn:\n\t\t:obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n\t\"\"\"\n\tdef lr_lambda(current_step):\n\t\t# Warmup\n\t\tif current_step < num_warmup_steps:\n\t\t\treturn float(current_step) / float(max(1, num_warmup_steps))\n\t\t# decadence\n\t\tprogress = float(current_step - num_warmup_steps) / float(\n\t\t\tmax(1, num_training_steps - num_warmup_steps)\n\t\t)\n\t\treturn max(\n\t\t\t0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n\t\t)\n\n\treturn LambdaLR(optimizer, lr_lambda, last_epoch)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:19:58.333233Z","iopub.execute_input":"2023-04-07T09:19:58.334199Z","iopub.status.idle":"2023-04-07T09:19:58.343357Z","shell.execute_reply.started":"2023-04-07T09:19:58.334157Z","shell.execute_reply":"2023-04-07T09:19:58.342149Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import torch\n\n\ndef model_fn(batch, model, criterion, device):\n\t\"\"\"Forward a batch through the model.\"\"\"\n\n\tmels, labels = batch\n\tmels = mels.to(device)\n\tlabels = labels.to(device)\n\n\touts = model(mels)\n\n\tloss = criterion(outs, labels)\n\n\t# Get the speaker id with highest probability.\n\tpreds = outs.argmax(1)\n\t# Compute accuracy.\n\taccuracy = torch.mean((preds == labels).float())\n\n\treturn loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:20:00.784379Z","iopub.execute_input":"2023-04-07T09:20:00.785067Z","iopub.status.idle":"2023-04-07T09:20:00.791621Z","shell.execute_reply.started":"2023-04-07T09:20:00.785028Z","shell.execute_reply":"2023-04-07T09:20:00.790464Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\n\ndef valid(dataloader, model, criterion, device): \n\t\"\"\"Validate on validation set.\"\"\"\n\n\tmodel.eval()\n\trunning_loss = 0.0\n\trunning_accuracy = 0.0\n\tpbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n\n\tfor i, batch in enumerate(dataloader):\n\t\twith torch.no_grad():\n\t\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n\t\t\trunning_loss += loss.item()\n\t\t\trunning_accuracy += accuracy.item()\n\n\t\tpbar.update(dataloader.batch_size)\n\t\tpbar.set_postfix(\n\t\t\tloss=f\"{running_loss / (i+1):.2f}\",\n\t\t\taccuracy=f\"{running_accuracy / (i+1):.2f}\",\n\t\t)\n\n\tpbar.close()\n\tmodel.train()\n\n\treturn running_accuracy / len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:20:02.584637Z","iopub.execute_input":"2023-04-07T09:20:02.585563Z","iopub.status.idle":"2023-04-07T09:20:02.594214Z","shell.execute_reply.started":"2023-04-07T09:20:02.585524Z","shell.execute_reply":"2023-04-07T09:20:02.593018Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, random_split\n\n\ndef parse_args():\n\t\"\"\"arguments\"\"\"\n\tconfig = {\n\t\t\"data_dir\": \"./Dataset\",\n\t\t\"save_path\": \"model.ckpt\",\n\t\t\"batch_size\": 32,\n\t\t\"n_workers\": 8,\n\t\t\"valid_steps\": 2000,\n\t\t\"warmup_steps\": 1000,\n\t\t\"save_steps\": 10000,\n\t\t\"total_steps\": 70000,\n\t}\n\n\treturn config\n\n\ndef main(\n\tdata_dir,\n\tsave_path,\n\tbatch_size,\n\tn_workers,\n\tvalid_steps,\n\twarmup_steps,\n\ttotal_steps,\n\tsave_steps,\n):\n\t\"\"\"Main function.\"\"\"\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\tprint(f\"[Info]: Use {device} now!\")\n\n\ttrain_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n\ttrain_iterator = iter(train_loader)\n\tprint(f\"[Info]: Finish loading data!\",flush = True)\n\n\tmodel = Classifier(n_spks=speaker_num).to(device)\n\tcriterion = nn.CrossEntropyLoss()\n\toptimizer = AdamW(model.parameters(), lr=1e-3)\n\tscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n\tprint(f\"[Info]: Finish creating model!\",flush = True)\n\n\tbest_accuracy = -1.0\n\tbest_state_dict = None\n\n\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n\tfor step in range(total_steps):\n\t\t# Get data\n\t\ttry:\n\t\t\tbatch = next(train_iterator)\n\t\texcept StopIteration:\n\t\t\ttrain_iterator = iter(train_loader)\n\t\t\tbatch = next(train_iterator)\n\n\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n\t\tbatch_loss = loss.item()\n\t\tbatch_accuracy = accuracy.item()\n\n\t\t# Updata model\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\tscheduler.step()\n\t\toptimizer.zero_grad()\n\n\t\t# Log\n\t\tpbar.update()\n\t\tpbar.set_postfix(\n\t\t\tloss=f\"{batch_loss:.2f}\",\n\t\t\taccuracy=f\"{batch_accuracy:.2f}\",\n\t\t\tstep=step + 1,\n\t\t)\n\n\t\t# Do validation\n\t\tif (step + 1) % valid_steps == 0:\n\t\t\tpbar.close()\n\n\t\t\tvalid_accuracy = valid(valid_loader, model, criterion, device)\n\n\t\t\t# keep the best model\n\t\t\tif valid_accuracy > best_accuracy:\n\t\t\t\tbest_accuracy = valid_accuracy\n\t\t\t\tbest_state_dict = model.state_dict()\n\n\t\t\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n\t\t# Save the best model so far.\n\t\tif (step + 1) % save_steps == 0 and best_state_dict is not None:\n\t\t\ttorch.save(best_state_dict, save_path)\n\t\t\tpbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n\tpbar.close()\n\n\nif __name__ == \"__main__\":\n\tmain(**parse_args())","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:22:27.600385Z","iopub.execute_input":"2023-04-07T09:22:27.600802Z","iopub.status.idle":"2023-04-07T09:26:07.175469Z","shell.execute_reply.started":"2023-04-07T09:22:27.600766Z","shell.execute_reply":"2023-04-07T09:26:07.168699Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n","output_type":"stream"},{"name":"stderr","text":"Train:   0% 0/2000 [06:55<?, ? step/s]\nTrain:   0% 0/2000 [04:37<?, ? step/s]\n","output_type":"stream"},{"name":"stdout","text":"[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:16<00:00, 26.09 step/s, accuracy=0.00, loss=5.51, step=2000]\nValid: 100% 5664/5667 [00:07<00:00, 719.78 uttr/s, accuracy=0.03, loss=5.42]\nTrain:  23% 465/2000 [00:13<00:45, 34.04 step/s, accuracy=0.03, loss=5.42, step=2465]Exception in thread Thread-12:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 49, in _pin_memory_loop\n    do_one_step()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 26, in do_one_step\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 113, in get\n    return _ForkingPickler.loads(res)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\", line 305, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/opt/conda/lib/python3.7/multiprocessing/resource_sharer.py\", line 57, in detach\n    with _resource_sharer.get_connection(self._id) as conn:\n  File \"/opt/conda/lib/python3.7/multiprocessing/resource_sharer.py\", line 87, in get_connection\n    c = Client(address, authkey=process.current_process().authkey)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 492, in Client\n    c = SocketClient(address)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 620, in SocketClient\n    s.connect(address)\nFileNotFoundError: [Errno 2] No such file or directory\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/3767169038.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_24/3767169038.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_dir, save_path, batch_size, n_workers, valid_steps, warmup_steps, total_steps, save_steps)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;31m# Updata model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[1;32m    487\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         )\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\n\n\nclass InferenceDataset(Dataset):\n\tdef __init__(self, data_dir):\n\t\ttestdata_path = Path(data_dir) / \"testdata.json\"\n\t\tmetadata = json.load(testdata_path.open())\n\t\tself.data_dir = data_dir\n\t\tself.data = metadata[\"utterances\"]\n\n\tdef __len__(self):\n\t\treturn len(self.data)\n\n\tdef __getitem__(self, index):\n\t\tutterance = self.data[index]\n\t\tfeat_path = utterance[\"feature_path\"]\n\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n\n\t\treturn feat_path, mel\n\n\ndef inference_collate_batch(batch):\n\t\"\"\"Collate a batch of data.\"\"\"\n\tfeat_paths, mels = zip(*batch)\n\n\treturn feat_paths, torch.stack(mels)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T08:58:17.440380Z","iopub.status.idle":"2023-04-07T08:58:17.441379Z","shell.execute_reply.started":"2023-04-07T08:58:17.441040Z","shell.execute_reply":"2023-04-07T08:58:17.441074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport csv\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\n\nimport torch\nfrom torch.utils.data import DataLoader\n\ndef parse_args():\n\t\"\"\"arguments\"\"\"\n\tconfig = {\n\t\t\"data_dir\": \"./Dataset\",\n\t\t\"model_path\": \"./model.ckpt\",\n\t\t\"output_path\": \"./output.csv\",\n\t}\n\n\treturn config\n\n\ndef main(\n\tdata_dir,\n\tmodel_path,\n\toutput_path,\n):\n\t\"\"\"Main function.\"\"\"\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\tprint(f\"[Info]: Use {device} now!\")\n\n\tmapping_path = Path(data_dir) / \"mapping.json\"\n\tmapping = json.load(mapping_path.open())\n\n\tdataset = InferenceDataset(data_dir)\n\tdataloader = DataLoader(\n\t\tdataset,\n\t\tbatch_size=1,\n\t\tshuffle=False,\n\t\tdrop_last=False,\n\t\tnum_workers=8,\n\t\tcollate_fn=inference_collate_batch,\n\t)\n\tprint(f\"[Info]: Finish loading data!\",flush = True)\n\n\tspeaker_num = len(mapping[\"id2speaker\"])\n\tmodel = Classifier(n_spks=speaker_num).to(device)\n\tmodel.load_state_dict(torch.load(model_path))\n\tmodel.eval()\n\tprint(f\"[Info]: Finish creating model!\",flush = True)\n\n\tresults = [[\"Id\", \"Category\"]]\n\tfor feat_paths, mels in tqdm(dataloader):\n\t\twith torch.no_grad():\n\t\t\tmels = mels.to(device)\n\t\t\touts = model(mels)\n\t\t\tpreds = outs.argmax(1).cpu().numpy()\n\t\t\tfor feat_path, pred in zip(feat_paths, preds):\n\t\t\t\tresults.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n\n\twith open(output_path, 'w', newline='') as csvfile:\n\t\twriter = csv.writer(csvfile)\n\t\twriter.writerows(results)\n\n\nif __name__ == \"__main__\":\n\tmain(**parse_args())","metadata":{"execution":{"iopub.status.busy":"2023-04-07T08:58:17.443046Z","iopub.status.idle":"2023-04-07T08:58:17.444778Z","shell.execute_reply.started":"2023-04-07T08:58:17.444480Z","shell.execute_reply":"2023-04-07T08:58:17.444513Z"},"trusted":true},"execution_count":null,"outputs":[]}]}